{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks\n",
    "\n",
    "Here we will go step by step building tensorflow's wide and deep algorithm using pytorch. Needless to say, all credit for the original algorithm to Heng-Tze Cheng et al. Their paper can be found [here](https://arxiv.org/pdf/1606.07792.pdf) and the tutorial [here](https://www.tensorflow.org/tutorials/wide_and_deep). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Prepare the data\n",
    "\n",
    "As I mentioned in demo1 all you have to do is, once your dataframe is in a form similar to `DF` here, set the experiment and run `prepare_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wide_deep.data_utils import prepare_data\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'\n",
    "\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target,scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wd_dataset` is a dictionary with all the neccesary information. Let's have a look to for example, the `train_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_dataset['train_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-The model\n",
    "\n",
    "The model is a combination of a linear classifier/regressor for sparse features (Wide) plus a neural network classifier/regressor that receives the embeddings. The figure below, taken from the [tutorial](https://www.tensorflow.org/tutorials/wide_and_deep), is a good illustration on how the algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "PATH = \"./image04.png\"\n",
    "Image(filename = PATH, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, \"all\" we have to do is:\n",
    "\n",
    "1. Prepare the wide part\n",
    "\n",
    "2. Prepare the deep part\n",
    "\n",
    "3. combine them\n",
    "\n",
    "So...let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_1. The Wide Part\n",
    "\n",
    "The wide part consist simply in the sparse features connected directly to the output neuron (or neurons if the problem is a multiclass classification). In the example here we will perform a logistic regression, so we need to connect the input features to an output neuron and use a *Sigmoid* activation function. \n",
    "\n",
    "In our case, this could be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_part = nn.Linear(wide_dim, n_class)\n",
    "\n",
    "print(wide_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we want our code to look pretty and be functional. When using pytorch, models are normally defined as classes (although if the are simple enough one could use the `Sequential` API) that inherit the methods from the `nn` module. Let's define the wide part properly and see how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide-side consists in simply in \"pluging\" the features into the output neuron(s)\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    wide_dim: int. Number of features per observation\n",
    "    method  : str. Regression, logistic or multiclass\n",
    "    n_class : int. number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self, wide_dim, n_class):\n",
    "\n",
    "        super(Wide, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.linear = nn.Linear(self.wide_dim, self.n_class)\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        out = torch.sigmoid(self.linear(X))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_model = Wide(wide_dim, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wide_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so far so good. We have simply created a model that consists of observations of 798 sparse features \"plugged\" into an output neuron that is activated with a *Sigmoid* function. Now, all we need to do is prepare the tensor to be passed through the model. Remember that the `prepare-data` function returns a dictionary with the wide and deep part arrays. Therefore, we can use the `train_dataset` to build the input tensor. \n",
    "\n",
    "Here the first column will be the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_dataset['train_dataset'].labels.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].wide])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set up the training: optimizer, loss function, etc.\n",
    "\n",
    "Pytorch provide a very handy functionality called `DataLoader`, at `torch.utils.data`. We will use if here to create the batches. Finally, the model needs to receive `Variables`. `Variables` support almost all operations you can perform on tensors. In addition, they define the computational graph, which will allow us later to automatically compute gradients. For more information read [here](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(wide_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "# from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_w = Variable(batch[:, 1:]).float()\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_model(X_w)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        \n",
    "        correct+= float((y_pred_cat == y).sum().item())\n",
    "    print (correct, total)\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.item(),3), round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_2. Deep part\n",
    "\n",
    "Things get a bit more \"colorful\" here. \n",
    "\n",
    "Still, the deep part implemented here will be comprised by two layers of 100 and 50 neurons, so strictly speaking and under today's standards, is not very \"deep\". \n",
    "\n",
    "As mentioned earlier, the deep part receives embeddings and can also receive numerical features if one likes. The set up of the deep part is \"stored\" in our favourite dictionar `wd_dataset`. There we have two entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wd_dataset['embeddings_input'])\n",
    "print(wd_dataset['deep_column_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should be read as follows: the feature `workclass` has 9 unique values and it will be represented using 10 embeddings. In addition, in the input tensor to the deep part, `workclass` is at column 2. With this information, plus continuous columns list at the beginning of this notebook, we can build the deep part of the model.\n",
    "\n",
    "In pytorch, embedding layers are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name, unique_vals, n_emb = wd_dataset['embeddings_input'][0]\n",
    "emb_layer = nn.Embedding(unique_vals, n_emb)\n",
    "print(emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep-side, which consists in a series of embeddings and numerical \n",
    "    features passed through a series of dense layers.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    embeddings_input (tuple): 3-elements tuple with the embeddings \"set-up\" -\n",
    "    (col_name, unique_values, embeddings dim)\n",
    "    continuous_cols (list) : list with the name of the continuum columns\n",
    "    deep_column_idx (dict) : dictionary where the keys are column names and the values\n",
    "    their corresponding index in the deep-side input tensor\n",
    "    hidden_layers (list) : list with the number of units per hidden layer\n",
    "    n_class (int) : number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,n_class):\n",
    "\n",
    "        super(Deep, self).__init__()\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        # build the embeddings that will be passed through the deep side    \n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        # the input dimension to the 1st hidden layer will be the sum of the\n",
    "        # embeddings dimensions plus the number of continuous features\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1], n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        out = torch.sigmoid(self.output(x_deep))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's built the model and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "hidden_layers = [100,50]\n",
    "deep_model = Deep(embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deep_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the deep part is comprised by: \n",
    "\n",
    "1. 5 embedding layers of dimensions 10, 10, 10, 8 and 10 respectively\n",
    "2. The 5 embedding layers will be contactenated with the two continuous feaures (age and hours per week) so that the first hidden layer will receive tensors of dimensions (?, 10+10+10+8+10+2=50)\n",
    "3. Two hidden layers of 100 and 50 neurons\n",
    "4. The output neuron with a Sigmoid activation function\n",
    "\n",
    "All left to do now is to build the input dataset and set up the training as we did before for the wide part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].deep])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(deep_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_d = Variable(batch[:, 1:])\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = deep_model(X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().item())\n",
    "        \n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.item(),3), round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep\n",
    "\n",
    "Time to combine the two parts. The code below is mostly identical to that of the `Deep` class. The two main differences are: \n",
    "\n",
    " 1. The output neuron receives now the wide and the deep side, so the input dimensions need to be adapted in both the definition of the model\n",
    " \n",
    "         self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "    \n",
    "    and the forward pass  \n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w], 1)\n",
    "\n",
    "\n",
    "Other than that, there are no major changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "\n",
    "    def __init__(self, wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class):\n",
    "\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "\n",
    "    def forward(self, X_w, X_d):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X_d[:,self.deep_column_idx[col]].long())\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X_d[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w.float()], 1)\n",
    "\n",
    "        out = torch.sigmoid(self.output(wide_deep_input))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model and have a look to what is \"inside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_deep_model = WideDeep(wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_deep_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, everything is identical, apart from the output layer, which now receives all the features from the wide side plus the 50 dense features from the deep side. \n",
    "\n",
    "Finally, the `dataset` parameter in the `DataLoader` method needs to have  `__getitem__` and `__len__` methods itself. Here we would like to build a loader that returns, per batch, the wide and deep tensors. Therefore, I coded a simple class that will facilitate the loading and make the code more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepLoader(Dataset):\n",
    "    \"\"\"Helper to facilitate loading the data to the pytorch models.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    data: namedtuple with 3 elements - (wide_input_data, deep_inp_data, target)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.X_wide = data.wide\n",
    "        self.X_deep = data.deep\n",
    "        self.Y = data.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xw = self.X_wide[idx]\n",
    "        xd = self.X_deep[idx]\n",
    "        y  = self.Y[idx]\n",
    "\n",
    "        return xw, xd, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "\n",
    "train_dataset = wd_dataset['train_dataset']\n",
    "widedeep_dataset = WideDeepLoader(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=widedeep_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(wide_deep_model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, (X_wide, X_deep, target) in enumerate(train_loader):\n",
    "        X_d = Variable(X_deep)\n",
    "        X_w = Variable(X_wide)\n",
    "        y = Variable(target).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_deep_model(X_w, X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().item())\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.item(),3), round(correct/total,4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it.\n",
    "\n",
    "The code in this demo, with some minor rings and bells, is wrapped-up into a the class `WideDeep` at `wide_deep.torch_model` so is easy to use. \n",
    "\n",
    "If you want to see how to use it simply go to demo3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
