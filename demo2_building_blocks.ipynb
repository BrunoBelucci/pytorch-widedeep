{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wide_deep.data_utils import prepare_data\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'\n",
    "\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_dataset(wide=array([[46, 50,  0, ...,  0,  0,  0],\n",
       "       [32, 45,  1, ...,  0,  0,  0],\n",
       "       [30, 30,  0, ...,  0,  0,  0],\n",
       "       ..., \n",
       "       [40, 40,  0, ...,  0,  0,  0],\n",
       "       [45, 37,  1, ...,  0,  0,  0],\n",
       "       [40, 45,  1, ...,  0,  0,  0]]), deep=array([[ 3,  1,  6, ...,  0, 46, 50],\n",
       "       [ 0,  0,  2, ...,  0, 32, 45],\n",
       "       [ 1,  4,  2, ...,  0, 30, 30],\n",
       "       ..., \n",
       "       [ 1,  0,  2, ...,  0, 40, 40],\n",
       "       [ 0,  1,  2, ...,  0, 45, 37],\n",
       "       [ 0,  1,  2, ...,  0, 40, 45]]), labels=array([1, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset['train_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    \"\"\"\n",
    "    Wide-side consists in simply in \"pluging\" the features into the output neuron(s)\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    wide_dim: int. Number of features per observation\n",
    "    method  : str. Regression, logistic or multiclass\n",
    "    n_class : int. number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self, wide_dim, n_class):\n",
    "\n",
    "        super(Wide, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.n_class = n_class\n",
    "\n",
    "        self.linear = nn.Linear(self.wide_dim, self.n_class)\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        out = F.sigmoid(self.linear(X))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class  = 1\n",
    "wide_model = Wide(wide_dim, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide (\n",
      "  (linear): Linear (798 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(wide_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34189, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset['train_dataset'].labels.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 46, 50, ...,  0,  0,  0],\n",
       "       [ 0, 32, 45, ...,  0,  0,  0],\n",
       "       [ 0, 30, 30, ...,  0,  0,  0],\n",
       "       ..., \n",
       "       [ 0, 40, 40, ...,  0,  0,  0],\n",
       "       [ 0, 45, 37, ...,  0,  0,  0],\n",
       "       [ 0, 40, 45, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].wide])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.427, accuracy: 0.8371\n",
      "Epoch 2 of 10, Loss: 0.501, accuracy: 0.8363\n",
      "Epoch 3 of 10, Loss: 0.414, accuracy: 0.8368\n",
      "Epoch 4 of 10, Loss: 0.183, accuracy: 0.8361\n",
      "Epoch 5 of 10, Loss: 0.37, accuracy: 0.8366\n",
      "Epoch 6 of 10, Loss: 0.345, accuracy: 0.837\n",
      "Epoch 7 of 10, Loss: 0.187, accuracy: 0.8366\n",
      "Epoch 8 of 10, Loss: 0.972, accuracy: 0.8379\n",
      "Epoch 9 of 10, Loss: 0.303, accuracy: 0.8376\n",
      "Epoch 10 of 10, Loss: 0.447, accuracy: 0.8373\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "# from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_w = Variable(batch[:, 1:]).float()\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_model(X_w)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data[0])\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.data[0],3), round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep-side, which consists in a series of embeddings and numerical \n",
    "    features passed through a series of dense layers.\n",
    "\n",
    "    Params:\n",
    "    --------\n",
    "    embeddings_input (tuple): 3-elements tuple with the embeddings \"set-up\" -\n",
    "    (col_name, unique_values, embeddings dim)\n",
    "    continuous_cols (list) : list with the name of the continuum columns\n",
    "    deep_column_idx (dict) : dictionary where the keys are column names and the values\n",
    "    their corresponding index in the deep-side input tensor\n",
    "    hidden_layers (list) : list with the number of units per hidden layer\n",
    "    n_class (int) : number of classes. Defaults to 1 if logistic or regression\n",
    "    \"\"\"\n",
    "    def __init__(self,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,n_class):\n",
    "\n",
    "        super(Deep, self).__init__()\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1], n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X[:,self.deep_column_idx[col]])\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        out = F.sigmoid(self.output(x_deep))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "hidden_layers = [100,50]\n",
    "deep_model = Deep(embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep (\n",
       "  (emb_layer_workclass): Embedding(9, 10)\n",
       "  (emb_layer_education): Embedding(16, 10)\n",
       "  (emb_layer_native_country): Embedding(42, 10)\n",
       "  (emb_layer_relationship): Embedding(6, 8)\n",
       "  (emb_layer_occupation): Embedding(15, 10)\n",
       "  (linear_1): Linear (50 -> 100)\n",
       "  (linear_2): Linear (100 -> 50)\n",
       "  (output): Linear (50 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  1, ...,  0, 46, 50],\n",
       "       [ 0,  0,  0, ...,  0, 32, 45],\n",
       "       [ 0,  1,  4, ...,  0, 30, 30],\n",
       "       ..., \n",
       "       [ 0,  1,  0, ...,  0, 40, 40],\n",
       "       [ 0,  0,  1, ...,  0, 45, 37],\n",
       "       [ 0,  0,  1, ...,  0, 40, 45]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = np.hstack([wd_dataset['train_dataset'].labels.reshape(-1, 1), wd_dataset['train_dataset'].deep])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.549, accuracy: 0.8149\n",
      "Epoch 2 of 10, Loss: 0.409, accuracy: 0.8342\n",
      "Epoch 3 of 10, Loss: 0.41, accuracy: 0.837\n",
      "Epoch 4 of 10, Loss: 0.177, accuracy: 0.8381\n",
      "Epoch 5 of 10, Loss: 0.15, accuracy: 0.8383\n",
      "Epoch 6 of 10, Loss: 0.519, accuracy: 0.8404\n",
      "Epoch 7 of 10, Loss: 0.383, accuracy: 0.8405\n",
      "Epoch 8 of 10, Loss: 0.348, accuracy: 0.8403\n",
      "Epoch 9 of 10, Loss: 0.701, accuracy: 0.8404\n",
      "Epoch 10 of 10, Loss: 0.175, accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(deep_model.parameters())\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "# from http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        X_d = Variable(batch[:, 1:])\n",
    "        y = Variable(batch[:, 0]).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = deep_model(X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data[0])\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.data[0],3), round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WideDeep(nn.Module):\n",
    "\n",
    "    def __init__(self, wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class):\n",
    "\n",
    "        super(WideDeep, self).__init__()\n",
    "        self.wide_dim = wide_dim\n",
    "        self.deep_column_idx = deep_column_idx\n",
    "        self.embeddings_input = embeddings_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.n_class = n_class\n",
    "\n",
    "        for col,val,dim in self.embeddings_input:\n",
    "            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))\n",
    "\n",
    "        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])\n",
    "        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])\n",
    "        for i,h in enumerate(self.hidden_layers[1:],1):\n",
    "            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, n_class)\n",
    "\n",
    "    def forward(self, X_w, X_d):\n",
    "\n",
    "        emb = [getattr(self, 'emb_layer_'+col)(X_d[:,self.deep_column_idx[col]])\n",
    "               for col,_,_ in self.embeddings_input]\n",
    "\n",
    "        cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]\n",
    "        cont = [X_d[:, cont_idx].float()]\n",
    "\n",
    "        deep_inp = torch.cat(emb+cont, 1)\n",
    "\n",
    "        x_deep = F.relu(self.linear_1(deep_inp))\n",
    "        for i in range(1,len(self.hidden_layers)):\n",
    "            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )\n",
    "\n",
    "        wide_deep_input = torch.cat([x_deep, X_w], 1)\n",
    "\n",
    "        out = F.sigmoid(self.output(wide_deep_input))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wide_deep_model = WideDeep(wide_dim, embeddings_input, continuous_cols, deep_column_idx, hidden_layers, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep (\n",
       "  (emb_layer_workclass): Embedding(9, 10)\n",
       "  (emb_layer_education): Embedding(16, 10)\n",
       "  (emb_layer_native_country): Embedding(42, 10)\n",
       "  (emb_layer_relationship): Embedding(6, 8)\n",
       "  (emb_layer_occupation): Embedding(15, 10)\n",
       "  (linear_1): Linear (50 -> 100)\n",
       "  (linear_2): Linear (100 -> 50)\n",
       "  (output): Linear (848 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_deep_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WideDeepLoader(Dataset):\n",
    "    \"\"\"Helper to facilitate loading the data to the pytorch models.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    data: namedtuple with 3 elements - (wide_input_data, deep_inp_data, target)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.X_wide = data.wide\n",
    "        self.X_deep = data.deep\n",
    "        self.Y = data.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        xw = self.X_wide[idx]\n",
    "        xd = self.X_deep[idx]\n",
    "        y  = self.Y[idx]\n",
    "\n",
    "        return xw, xd, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "\n",
    "train_dataset = wd_dataset['train_dataset']\n",
    "widedeep_dataset = WideDeepLoader(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=widedeep_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.532, accuracy: 0.8176\n",
      "Epoch 2 of 10, Loss: 0.238, accuracy: 0.8355\n",
      "Epoch 3 of 10, Loss: 0.382, accuracy: 0.838\n",
      "Epoch 4 of 10, Loss: 0.659, accuracy: 0.8372\n",
      "Epoch 5 of 10, Loss: 0.26, accuracy: 0.8396\n",
      "Epoch 6 of 10, Loss: 0.435, accuracy: 0.8411\n",
      "Epoch 7 of 10, Loss: 0.326, accuracy: 0.8387\n",
      "Epoch 8 of 10, Loss: 0.378, accuracy: 0.8416\n",
      "Epoch 9 of 10, Loss: 0.13, accuracy: 0.8426\n",
      "Epoch 10 of 10, Loss: 0.267, accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(wide_deep_model.parameters())\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i, (X_wide, X_deep, target) in enumerate(train_loader):\n",
    "        X_d = Variable(X_deep)\n",
    "        X_w = Variable(X_wide).float()\n",
    "        y = Variable(target).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = wide_deep_model(X_w, X_d)\n",
    "        loss = F.binary_cross_entropy(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total+= y.size(0)\n",
    "        y_pred_cat = (y_pred > 0.5).squeeze(1).float()\n",
    "        correct+= float((y_pred_cat == y).sum().data[0])\n",
    "\n",
    "    print ('Epoch {} of {}, Loss: {}, accuracy: {}'.format(epoch+1,\n",
    "        n_epochs, round(loss.data[0],3), round(correct/total,4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
