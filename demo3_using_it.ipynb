{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the model\n",
    "\n",
    "To understand the model it would be convenient if you have gone through demo1 and 2, however can learn how to use the model simply reading this notebook. \n",
    "\n",
    "I will use 3 examples to illustrate the different set-ups that can be used with this pytorch implementation of wide and deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load the data\n",
    "\n",
    "Note that, as long as your dataset is in a state similar to that of `adult_data.csv` below (remove NaN, impute missing values, etc..), you are \"good to go\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \\\n",
       "0          2174             0              40  United-States          <=50K   \n",
       "1             0             0              13  United-States          <=50K   \n",
       "2             0             0              40  United-States          <=50K   \n",
       "3             0             0              40  United-States          <=50K   \n",
       "4             0             0              40           Cuba          <=50K   \n",
       "\n",
       "   income_label  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression with varying embedding dimensions and no dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_1. Set the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define a target for logistic regression:\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "# Experiment set up\n",
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_2. prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wide_deep.data_utils import prepare_data\n",
    "\n",
    "#Â just call prepare_data\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network set up\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=1 # for logistic and regression\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = None\n",
    "\n",
    "# Build the model. Again you just need to call WideDeep\n",
    "from wide_deep.torch_model import WideDeep\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers, dropout, encoding_dict,n_class)\n",
    "\n",
    "# I have included a compile method if you want to change the fitting method or the optimizer\n",
    "model.compile(method=method, optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep (\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (linear_1): Linear (50 -> 100)\n",
      "  (linear_2): Linear (100 -> 50)\n",
      "  (output): Linear (848 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_4. Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/usr/local/lib/python2.7/site-packages/torch/nn/functional.py:767: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.425, accuracy: 0.8086\n",
      "Epoch 2 of 10, Loss: 0.173, accuracy: 0.8361\n",
      "Epoch 3 of 10, Loss: 0.251, accuracy: 0.8384\n",
      "Epoch 4 of 10, Loss: 0.237, accuracy: 0.8405\n",
      "Epoch 5 of 10, Loss: 0.112, accuracy: 0.8404\n",
      "Epoch 6 of 10, Loss: 0.188, accuracy: 0.8413\n",
      "Epoch 7 of 10, Loss: 0.074, accuracy: 0.8423\n",
      "Epoch 8 of 10, Loss: 0.17, accuracy: 0.8432\n",
      "Epoch 9 of 10, Loss: 0.228, accuracy: 0.8428\n",
      "Epoch 10 of 10, Loss: 0.376, accuracy: 0.8439\n",
      "0.837029959735\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "\n",
    "# As your usual Sklearn model, simply call fit/predict\n",
    "model.fit(dataset=train_dataset, n_epochs=10, batch_size=64)\n",
    "pred = model.predict(dataset=test_dataset)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, test_dataset.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have included a method to easily get the learned embeddings. This will return a dictionary where the keys are the column values and the values are the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10th': array([ 1.0558697 , -0.10497121,  1.2519902 , -1.20969331, -0.37003803,\n",
       "         0.26222366,  1.39537013,  0.66922128, -1.14872277, -1.66497922], dtype=float32),\n",
       " '11th': array([ 0.76582593, -0.15720901, -0.79173702,  0.17092067, -1.01140571,\n",
       "        -0.15254961,  1.59629261, -1.03472006, -0.1246258 ,  0.87272727], dtype=float32),\n",
       " '12th': array([ 2.80748963, -0.40501541, -1.66380119,  1.119385  ,  0.11228444,\n",
       "         0.46560571, -0.2575815 , -0.78553766,  0.40721282,  2.17365384], dtype=float32),\n",
       " '1st-4th': array([-0.59988064, -0.91489893,  0.77964532,  1.34235549, -2.21585774,\n",
       "        -1.20931304,  1.87390292,  0.40189996, -1.43448257,  0.0121912 ], dtype=float32),\n",
       " '5th-6th': array([ 0.13168913,  0.50879979,  0.44774669,  0.75261694, -2.11371017,\n",
       "        -0.86445326, -0.59014183, -1.84488511, -0.8879115 , -0.68353879], dtype=float32),\n",
       " '7th-8th': array([ 1.42483819,  0.34507382, -0.05195802,  1.38898981,  0.17512439,\n",
       "        -0.58219528,  0.94600356, -0.67991239, -1.80070949, -0.68990695], dtype=float32),\n",
       " '9th': array([ 0.84937495,  0.18928385, -0.8980329 ,  1.20929003,  0.22811069,\n",
       "         0.35240394,  0.84941047, -0.69901848,  0.07588249, -0.27054811], dtype=float32),\n",
       " 'Assoc-acdm': array([ 0.05810629, -1.25012755, -1.05227268, -0.00666486,  0.88830411,\n",
       "         0.50737596,  0.67054886,  0.26397765, -0.09015059,  0.44837326], dtype=float32),\n",
       " 'Assoc-voc': array([ 1.01582098,  0.40545571,  0.96072149,  0.17280895, -0.12402227,\n",
       "         0.0368996 ,  0.57116669,  1.57069802, -0.2876817 ,  0.8799817 ], dtype=float32),\n",
       " 'Bachelors': array([ 0.35576788,  0.18159543,  0.07858612,  1.12478256,  0.12776014,\n",
       "         0.41710249, -1.09058726,  1.38790727,  0.34605154, -0.70506179], dtype=float32),\n",
       " 'Doctorate': array([-2.7622664 ,  1.56626964,  0.48016456, -0.16346474,  1.68042314,\n",
       "         0.9269141 , -0.79821414, -1.53146839,  1.99243569,  0.9012208 ], dtype=float32),\n",
       " 'HS-grad': array([ 0.06493477, -1.69434929, -0.108916  , -0.44833779,  1.59829664,\n",
       "         0.19638543,  0.98757291, -0.75816447, -2.88351798, -0.03027572], dtype=float32),\n",
       " 'Masters': array([-1.17790508,  2.30469227,  2.46537971, -0.02742275,  0.41417554,\n",
       "         0.50062221,  0.6047889 ,  0.65633202,  1.04308689, -0.82801151], dtype=float32),\n",
       " 'Preschool': array([ 0.46218312, -1.3821547 , -1.45895326,  1.00207102, -2.59209466,\n",
       "        -2.07303119,  0.87239748, -0.24926367, -1.40725338,  0.19515684], dtype=float32),\n",
       " 'Prof-school': array([-1.64041114, -0.00505874,  0.10719328, -0.18879749,  1.84211004,\n",
       "        -1.14806819, -1.57541323,  0.30641121, -0.08031298,  0.96996194], dtype=float32),\n",
       " 'Some-college': array([-0.55294096, -0.76894253,  1.35381234,  0.36266825, -1.00209892,\n",
       "         0.81457275, -0.71004349, -0.8900364 ,  0.10882758, -1.56127894], dtype=float32)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_embeddings('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiclass classification with fixed embedding dimensions (10) and varying dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a feature for multiclass classification. Note that **this is only for illustration purposes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep (\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (linear_1): Linear (49 -> 100)\n",
      "  (linear_1_drop): Dropout (p = 0.5)\n",
      "  (linear_2): Linear (100 -> 50)\n",
      "  (linear_2_drop): Dropout (p = 0.2)\n",
      "  (output): Linear (847 -> 3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's define age groups\n",
    "age_groups = [0, 25, 50, 90]\n",
    "age_labels = range(len(age_groups) - 1)\n",
    "DF['age_group'] = pd.cut(DF['age'], age_groups, labels=age_labels)\n",
    "\n",
    "# Set the experiment\n",
    "wide_cols = ['hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols  = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols  = ['education', 'relationship','workclass','occupation','native_country']\n",
    "continuous_cols = [\"hours_per_week\"]\n",
    "target = 'age_group'\n",
    "method = 'multiclass'\n",
    "\n",
    "wd_dataset = prepare_data(DF,wide_cols,crossed_cols,embeddings_cols,continuous_cols,target,def_dim=10)\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=3\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = [0.5, 0.2]\n",
    "\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,dropout,encoding_dict,n_class)\n",
    "model.compile(method=method)\n",
    "\n",
    "# Let's have a look to the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.964, accuracy: 0.6522\n",
      "Epoch 2 of 10, Loss: 1.013, accuracy: 0.6829\n",
      "Epoch 3 of 10, Loss: 0.992, accuracy: 0.6873\n",
      "Epoch 4 of 10, Loss: 0.991, accuracy: 0.69\n",
      "Epoch 5 of 10, Loss: 1.024, accuracy: 0.693\n",
      "Epoch 6 of 10, Loss: 0.706, accuracy: 0.6933\n",
      "Epoch 7 of 10, Loss: 0.833, accuracy: 0.6959\n",
      "Epoch 8 of 10, Loss: 0.76, accuracy: 0.6958\n",
      "Epoch 9 of 10, Loss: 0.783, accuracy: 0.6971\n",
      "Epoch 10 of 10, Loss: 0.898, accuracy: 0.698\n",
      "\n",
      " [[  9.97471273e-01   2.52866116e-03   4.56306566e-08]\n",
      " [  9.44395465e-11   1.00000000e+00   5.53709922e-09]\n",
      " [  1.76757031e-09   9.99999881e-01   9.40417166e-08]\n",
      " ..., \n",
      " [  3.58941092e-04   9.91232693e-01   8.40835553e-03]\n",
      " [  3.10289147e-06   9.99976993e-01   1.99342994e-05]\n",
      " [  5.78610539e-01   4.08240706e-01   1.31487865e-02]]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "model.fit(dataset=train_dataset, n_epochs=10, batch_size=64)\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "\n",
    "# The model object also has a predict_proba method in case you want probabilities instead of class\n",
    "pred = model.predict_proba(test_dataset)\n",
    "print('\\n {}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.735653027645\n",
      "\n",
      " 0.703610182215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"\\n {}\".format(f1_score(model.predict(test_dataset), test_dataset.labels, average=\"weighted\")))\n",
    "\n",
    "print(\"\\n {}\".format(accuracy_score(model.predict(test_dataset), test_dataset.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression with varying embedding dimensions and varying dropout\n",
    "\n",
    "Again, bear in mind that here we use `age` as target just **for illustration purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep (\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (linear_1): Linear (49 -> 100)\n",
      "  (linear_1_drop): Dropout (p = 0.5)\n",
      "  (linear_2): Linear (100 -> 50)\n",
      "  (linear_2_drop): Dropout (p = 0.2)\n",
      "  (output): Linear (847 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment\n",
    "wide_cols = ['hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols  = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols  = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"hours_per_week\"]\n",
    "target = 'age'\n",
    "method = 'regression'\n",
    "\n",
    "# Prepare the dataset\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target)\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=1\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = [0.5, 0.2]\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,dropout,encoding_dict,n_class)\n",
    "model.compile(method=method)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 293.391\n",
      "Epoch 2 of 10, Loss: 190.727\n",
      "Epoch 3 of 10, Loss: 229.331\n",
      "Epoch 4 of 10, Loss: 186.07\n",
      "Epoch 5 of 10, Loss: 192.077\n",
      "Epoch 6 of 10, Loss: 59.602\n",
      "Epoch 7 of 10, Loss: 178.112\n",
      "Epoch 8 of 10, Loss: 137.38\n",
      "Epoch 9 of 10, Loss: 135.515\n",
      "Epoch 10 of 10, Loss: 66.123\n",
      "\n",
      " RMSE: 11.2608167188\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "model.fit(dataset=train_dataset, n_epochs=10, batch_size=64)\n",
    "\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "pred = model.predict(test_dataset)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n RMSE: {}\".format(np.sqrt(mean_squared_error(pred, test_dataset.labels))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
