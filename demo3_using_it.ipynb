{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the model\n",
    "\n",
    "To understand the model it would be convenient if you have gone through demo1 and 2, however you can learn how to use the model simply reading this notebook. \n",
    "\n",
    "I will use 3 examples to illustrate the different set-ups that can be used with this pytorch implementation of wide and deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load the data\n",
    "\n",
    "Note that, as long as your dataset is in a state similar to that of `adult_data.csv` below (remove NaN, impute missing values, etc..), you are \"good to go\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \\\n",
       "0          2174             0              40  United-States          <=50K   \n",
       "1             0             0              13  United-States          <=50K   \n",
       "2             0             0              40  United-States          <=50K   \n",
       "3             0             0              40  United-States          <=50K   \n",
       "4             0             0              40           Cuba          <=50K   \n",
       "\n",
       "   income_label  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression with varying embedding dimensions, no dropout and Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_1. Set the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a target for logistic regression:\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "# Experiment set up\n",
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_2. prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wide_deep.data_utils import prepare_data\n",
    "\n",
    "#Â just call prepare_data\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target,scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_3. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network set up\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=1 # for logistic and regression\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = None\n",
    "\n",
    "# Build the model. Again you just need to call WideDeep\n",
    "from wide_deep.torch_model import WideDeep\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers, dropout, encoding_dict,n_class)\n",
    "\n",
    "# I have included a compile method if you want to change the fitting method or the optimizer\n",
    "model.compile(method=method, optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep(\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (linear_1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (output): Linear(in_features=848, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1_4. Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 0.136, accuracy: 0.8246\n",
      "Epoch 2 of 10, Loss: 0.106, accuracy: 0.8392\n",
      "Epoch 3 of 10, Loss: 0.513, accuracy: 0.8421\n",
      "Epoch 4 of 10, Loss: 0.345, accuracy: 0.8414\n",
      "Epoch 5 of 10, Loss: 0.29, accuracy: 0.843\n",
      "Epoch 6 of 10, Loss: 0.227, accuracy: 0.8443\n",
      "Epoch 7 of 10, Loss: 0.426, accuracy: 0.845\n",
      "Epoch 8 of 10, Loss: 0.183, accuracy: 0.8454\n",
      "Epoch 9 of 10, Loss: 0.322, accuracy: 0.8461\n",
      "Epoch 10 of 10, Loss: 0.246, accuracy: 0.8469\n",
      "0.8382583771241384\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "\n",
    "# As your usual Sklearn model, simply call fit/predict\n",
    "model.fit(dataset=train_dataset, n_epochs=10, batch_size=64)\n",
    "pred = model.predict(dataset=test_dataset)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(pred, test_dataset.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have included a method to easily get the learned embeddings. This will return a dictionary where the keys are the column values and the values are the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bachelors': array([-1.1927266 ,  0.13337217,  0.751513  , -0.3854133 , -1.512503  ,\n",
       "         0.43075648,  0.03185017,  0.2740599 , -1.3502986 , -0.51524764],\n",
       "       dtype=float32),\n",
       " 'HS-grad': array([ 0.01510752, -0.41036212, -1.2737428 , -0.03190449,  0.30465913,\n",
       "        -0.4891645 , -0.35087353,  1.7667191 ,  0.90333945, -0.42637545],\n",
       "       dtype=float32),\n",
       " '11th': array([-1.3361819 , -1.0304003 , -0.7671982 ,  1.1118906 ,  0.6290409 ,\n",
       "         0.09973534, -0.41261104, -0.79101914,  1.2672484 ,  0.7189385 ],\n",
       "       dtype=float32),\n",
       " 'Masters': array([ 0.5837133 , -1.3451334 ,  0.9863935 ,  0.35932744, -0.13541682,\n",
       "         0.34770364, -0.8982047 ,  0.4550249 , -1.326133  , -0.08214497],\n",
       "       dtype=float32),\n",
       " '9th': array([ 0.00944321, -0.2883264 ,  1.1186845 ,  0.16699162,  0.20891678,\n",
       "        -2.222243  ,  0.90257394, -2.499814  ,  0.32215422, -0.02830464],\n",
       "       dtype=float32),\n",
       " 'Some-college': array([ 0.11737815, -0.9354352 , -1.6950701 , -0.3879866 , -0.34800476,\n",
       "         0.65498114, -1.0632497 ,  1.2390918 , -1.3980893 , -1.5068939 ],\n",
       "       dtype=float32),\n",
       " 'Assoc-acdm': array([-0.3248521 ,  0.67525595, -0.7607256 , -1.688361  , -0.01024881,\n",
       "        -0.17185631, -1.5726321 , -0.33589116, -0.6568722 , -0.83356154],\n",
       "       dtype=float32),\n",
       " 'Assoc-voc': array([ 0.11711311,  1.6658193 ,  0.2525636 , -1.7053522 ,  0.11374688,\n",
       "         0.69635576,  0.39209226,  0.55386406,  1.4460421 , -0.4076955 ],\n",
       "       dtype=float32),\n",
       " '7th-8th': array([ 0.8109543 , -0.9696295 , -1.1880634 , -2.673678  ,  1.387889  ,\n",
       "         0.03207216,  0.28635803,  0.32005164, -0.14126171, -0.12705447],\n",
       "       dtype=float32),\n",
       " 'Doctorate': array([ 2.5456786 ,  0.9495662 , -0.65327275,  0.63417935, -1.4665067 ,\n",
       "        -1.0520831 , -0.8822009 ,  1.7168643 ,  1.3397688 ,  1.0705113 ],\n",
       "       dtype=float32),\n",
       " 'Prof-school': array([-0.3236308 ,  0.36975744,  0.79298687, -0.24033554,  0.8012961 ,\n",
       "        -0.38213903,  0.20259416, -0.30737472, -2.190927  ,  0.47054496],\n",
       "       dtype=float32),\n",
       " '5th-6th': array([-0.71498626, -1.3042029 ,  0.04956457, -0.20074964,  0.85997975,\n",
       "         2.4887364 ,  0.9329344 , -0.33221987, -0.37141427,  1.9041626 ],\n",
       "       dtype=float32),\n",
       " '10th': array([-0.5197668 , -1.2800047 ,  1.5472891 , -1.141539  ,  0.00724531,\n",
       "         1.3354197 ,  1.4840577 ,  0.9995618 , -0.03808165, -1.1237134 ],\n",
       "       dtype=float32),\n",
       " '1st-4th': array([ 1.1701114 , -1.0981313 , -1.5367142 ,  0.16519445, -0.0972092 ,\n",
       "        -0.3711076 ,  0.9954778 , -0.94091356, -0.75837976, -1.9332327 ],\n",
       "       dtype=float32),\n",
       " 'Preschool': array([-0.7313262 , -0.56184304,  0.30143896,  0.8417214 ,  2.0694172 ,\n",
       "        -1.2695692 ,  1.461705  , -0.6897159 ,  1.6769298 ,  0.55851436],\n",
       "       dtype=float32),\n",
       " '12th': array([-1.2723062 , -0.27862272, -0.3878713 , -0.9044023 , -0.00804312,\n",
       "        -1.1498355 ,  1.0327121 ,  0.29477796,  0.2951289 ,  0.96019965],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_embeddings('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiclass classification with fixed embedding dimensions (10), varying dropout and RMSProp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a feature for multiclass classification. Note that **this is only for illustration purposes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep(\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 10)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (linear_1): Linear(in_features=51, out_features=100, bias=True)\n",
      "  (linear_1_drop): Dropout(p=0.5)\n",
      "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear_2_drop): Dropout(p=0.2)\n",
      "  (output): Linear(in_features=847, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's define age groups\n",
    "age_groups = [0, 25, 50, 90]\n",
    "age_labels = range(len(age_groups) - 1)\n",
    "DF['age_group'] = pd.cut(DF['age'], age_groups, labels=age_labels)\n",
    "\n",
    "# Set the experiment\n",
    "wide_cols = ['hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = ['education', 'relationship','workclass','occupation','native_country']\n",
    "continuous_cols = [\"hours_per_week\"]\n",
    "target = 'age_group'\n",
    "method = 'multiclass'\n",
    "\n",
    "wd_dataset = prepare_data(DF,wide_cols,crossed_cols,embeddings_cols,continuous_cols,target,scale=True,def_dim=10)\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=3\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = [0.5, 0.2]\n",
    "\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,dropout,encoding_dict,n_class)\n",
    "model.compile(method=method, optimizer=\"RMSprop\")\n",
    "\n",
    "# Let's have a look to the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10, Loss: 1.131, accuracy: 0.6735\n",
      "Epoch 2 of 10, Loss: 0.846, accuracy: 0.6843\n",
      "Epoch 3 of 10, Loss: 0.902, accuracy: 0.686\n",
      "Epoch 4 of 10, Loss: 0.806, accuracy: 0.691\n",
      "Epoch 5 of 10, Loss: 1.015, accuracy: 0.6931\n",
      "Epoch 6 of 10, Loss: 0.77, accuracy: 0.694\n",
      "Epoch 7 of 10, Loss: 0.868, accuracy: 0.6962\n",
      "Epoch 8 of 10, Loss: 0.808, accuracy: 0.6973\n",
      "Epoch 9 of 10, Loss: 0.977, accuracy: 0.6972\n",
      "Epoch 10 of 10, Loss: 0.851, accuracy: 0.6968\n",
      "\n",
      " [[9.9808323e-01 1.9167198e-03 1.2708337e-07]\n",
      " [1.8705309e-12 1.0000000e+00 1.0048575e-09]\n",
      " [2.1682714e-08 9.9999905e-01 9.1604261e-07]\n",
      " ...\n",
      " [1.0082698e-03 9.6010476e-01 3.8887005e-02]\n",
      " [2.4448596e-07 9.9994826e-01 5.1442345e-05]\n",
      " [6.8863249e-01 3.0600473e-01 5.3628702e-03]]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "model.fit(dataset=train_dataset, n_epochs=10, batch_size=64)\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "\n",
    "# The model object also has a predict_proba method in case you want probabilities instead of class\n",
    "pred = model.predict_proba(test_dataset)\n",
    "print('\\n {}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7318796365288384\n",
      "\n",
      " 0.7006756295639118\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"\\n {}\".format(f1_score(model.predict(test_dataset), test_dataset.labels, average=\"weighted\")))\n",
    "\n",
    "print(\"\\n {}\".format(accuracy_score(model.predict(test_dataset), test_dataset.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression with varying embedding dimensions and varying dropout.\n",
    "\n",
    "Again, bear in mind that here we use `age` as target just **for illustration purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideDeep(\n",
      "  (emb_layer_native_country): Embedding(42, 10)\n",
      "  (emb_layer_relationship): Embedding(6, 8)\n",
      "  (emb_layer_occupation): Embedding(15, 10)\n",
      "  (emb_layer_education): Embedding(16, 10)\n",
      "  (emb_layer_workclass): Embedding(9, 10)\n",
      "  (linear_1): Linear(in_features=49, out_features=100, bias=True)\n",
      "  (linear_1_drop): Dropout(p=0.5)\n",
      "  (linear_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (linear_2_drop): Dropout(p=0.2)\n",
      "  (output): Linear(in_features=847, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the experiment\n",
    "wide_cols = ['hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols  = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols  = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',10)]\n",
    "continuous_cols = [\"hours_per_week\"]\n",
    "target = 'age'\n",
    "method = 'regression'\n",
    "\n",
    "# Prepare the dataset\n",
    "wd_dataset = prepare_data(DF, wide_cols,crossed_cols,embeddings_cols,continuous_cols,target)\n",
    "\n",
    "wide_dim = wd_dataset['train_dataset'].wide.shape[1]\n",
    "n_class=1\n",
    "deep_column_idx = wd_dataset['deep_column_idx']\n",
    "embeddings_input= wd_dataset['embeddings_input']\n",
    "encoding_dict   = wd_dataset['encoding_dict']\n",
    "hidden_layers = [100,50]\n",
    "dropout = [0.5, 0.2]\n",
    "model = WideDeep(wide_dim,embeddings_input,continuous_cols,deep_column_idx,hidden_layers,dropout,encoding_dict,n_class)\n",
    "model.compile(method=method)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100, Loss: 246.073\n",
      "Epoch 2 of 100, Loss: 94.48\n",
      "Epoch 3 of 100, Loss: 126.0\n",
      "Epoch 4 of 100, Loss: 115.982\n",
      "Epoch 5 of 100, Loss: 133.325\n",
      "Epoch 6 of 100, Loss: 158.583\n",
      "Epoch 7 of 100, Loss: 54.36\n",
      "Epoch 8 of 100, Loss: 79.254\n",
      "Epoch 9 of 100, Loss: 182.114\n",
      "Epoch 10 of 100, Loss: 51.386\n",
      "Epoch 11 of 100, Loss: 104.073\n",
      "Epoch 12 of 100, Loss: 143.09\n",
      "Epoch 13 of 100, Loss: 70.531\n",
      "Epoch 14 of 100, Loss: 97.966\n",
      "Epoch 15 of 100, Loss: 59.099\n",
      "Epoch 16 of 100, Loss: 94.067\n",
      "Epoch 17 of 100, Loss: 183.913\n",
      "Epoch 18 of 100, Loss: 47.979\n",
      "Epoch 19 of 100, Loss: 179.66\n",
      "Epoch 20 of 100, Loss: 137.305\n",
      "Epoch 21 of 100, Loss: 127.122\n",
      "Epoch 22 of 100, Loss: 229.17\n",
      "Epoch 23 of 100, Loss: 151.097\n",
      "Epoch 24 of 100, Loss: 161.884\n",
      "Epoch 25 of 100, Loss: 128.496\n",
      "Epoch 26 of 100, Loss: 157.841\n",
      "Epoch 27 of 100, Loss: 369.787\n",
      "Epoch 28 of 100, Loss: 94.269\n",
      "Epoch 29 of 100, Loss: 134.898\n",
      "Epoch 30 of 100, Loss: 74.488\n",
      "Epoch 31 of 100, Loss: 214.186\n",
      "Epoch 32 of 100, Loss: 112.555\n",
      "Epoch 33 of 100, Loss: 78.977\n",
      "Epoch 34 of 100, Loss: 134.299\n",
      "Epoch 35 of 100, Loss: 35.186\n",
      "Epoch 36 of 100, Loss: 76.879\n",
      "Epoch 37 of 100, Loss: 195.191\n",
      "Epoch 38 of 100, Loss: 113.689\n",
      "Epoch 39 of 100, Loss: 104.923\n",
      "Epoch 40 of 100, Loss: 155.706\n",
      "Epoch 41 of 100, Loss: 115.923\n",
      "Epoch 42 of 100, Loss: 67.577\n",
      "Epoch 43 of 100, Loss: 105.407\n",
      "Epoch 44 of 100, Loss: 125.645\n",
      "Epoch 45 of 100, Loss: 133.77\n",
      "Epoch 46 of 100, Loss: 149.836\n",
      "Epoch 47 of 100, Loss: 173.448\n",
      "Epoch 48 of 100, Loss: 110.101\n",
      "Epoch 49 of 100, Loss: 138.044\n",
      "Epoch 50 of 100, Loss: 153.611\n",
      "Epoch 51 of 100, Loss: 61.501\n",
      "Epoch 52 of 100, Loss: 71.998\n",
      "Epoch 53 of 100, Loss: 188.658\n",
      "Epoch 54 of 100, Loss: 80.28\n",
      "Epoch 55 of 100, Loss: 95.933\n",
      "Epoch 56 of 100, Loss: 136.835\n",
      "Epoch 57 of 100, Loss: 129.396\n",
      "Epoch 58 of 100, Loss: 66.494\n",
      "Epoch 59 of 100, Loss: 111.14\n",
      "Epoch 60 of 100, Loss: 176.363\n",
      "Epoch 61 of 100, Loss: 75.573\n",
      "Epoch 62 of 100, Loss: 53.56\n",
      "Epoch 63 of 100, Loss: 89.78\n",
      "Epoch 64 of 100, Loss: 93.642\n",
      "Epoch 65 of 100, Loss: 105.566\n",
      "Epoch 66 of 100, Loss: 159.151\n",
      "Epoch 67 of 100, Loss: 89.265\n",
      "Epoch 68 of 100, Loss: 121.349\n",
      "Epoch 69 of 100, Loss: 98.278\n",
      "Epoch 70 of 100, Loss: 249.79\n",
      "Epoch 71 of 100, Loss: 111.768\n",
      "Epoch 72 of 100, Loss: 137.894\n",
      "Epoch 73 of 100, Loss: 131.131\n",
      "Epoch 74 of 100, Loss: 83.891\n",
      "Epoch 75 of 100, Loss: 99.166\n",
      "Epoch 76 of 100, Loss: 219.011\n",
      "Epoch 77 of 100, Loss: 62.321\n",
      "Epoch 78 of 100, Loss: 109.47\n",
      "Epoch 79 of 100, Loss: 122.896\n",
      "Epoch 80 of 100, Loss: 58.683\n",
      "Epoch 81 of 100, Loss: 199.865\n",
      "Epoch 82 of 100, Loss: 122.807\n",
      "Epoch 83 of 100, Loss: 138.183\n",
      "Epoch 84 of 100, Loss: 152.517\n",
      "Epoch 85 of 100, Loss: 34.736\n",
      "Epoch 86 of 100, Loss: 34.167\n",
      "Epoch 87 of 100, Loss: 152.289\n",
      "Epoch 88 of 100, Loss: 58.821\n",
      "Epoch 89 of 100, Loss: 70.747\n",
      "Epoch 90 of 100, Loss: 259.394\n",
      "Epoch 91 of 100, Loss: 79.002\n",
      "Epoch 92 of 100, Loss: 80.815\n",
      "Epoch 93 of 100, Loss: 152.681\n",
      "Epoch 94 of 100, Loss: 103.209\n",
      "Epoch 95 of 100, Loss: 75.965\n",
      "Epoch 96 of 100, Loss: 67.907\n",
      "Epoch 97 of 100, Loss: 96.509\n",
      "Epoch 98 of 100, Loss: 182.213\n",
      "Epoch 99 of 100, Loss: 55.769\n",
      "Epoch 100 of 100, Loss: 115.025\n",
      "\n",
      " RMSE: 11.100189228700662\n"
     ]
    }
   ],
   "source": [
    "train_dataset = wd_dataset['train_dataset']\n",
    "model.fit(dataset=train_dataset, n_epochs=100, batch_size=64)\n",
    "\n",
    "test_dataset  = wd_dataset['test_dataset']\n",
    "pred = model.predict(test_dataset)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"\\n RMSE: {}\".format(np.sqrt(mean_squared_error(pred, test_dataset.labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
