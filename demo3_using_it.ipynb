{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the model\n",
    "\n",
    "To understand the model it would be convenient if you have gone through demo1 and 2, however you can learn how to use the model simply reading this notebook.\n",
    "\n",
    "### 0. Load the data\n",
    "\n",
    "Note that, as long as your dataset is in a state similar to that of adult.csv or listings.csv (after `airbnb_data_preprocessing.py`), i.e. remove NaN, impute missing values, etc..., you are \"good to go\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prepare_data import prepare_data_adult, prepare_data_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Logistic regression with the adult dataset\n",
    "\n",
    "### 1.1 Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_label</th>\n",
       "      <th>age_buckets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational_num      marital_status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country  income_label age_buckets  \n",
       "0              40  United-States             0           0  \n",
       "1              50  United-States             0           3  \n",
       "2              40  United-States             1           1  \n",
       "3              40  United-States             1           4  \n",
       "4              30  United-States             0         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following will all happen if you simply run: python prepare_data.py --dataset adult\n",
    "DF_adult = pd.read_csv(DATA_PATH/'adult/adult.csv')\n",
    "DF_adult.columns = [c.replace(\"-\", \"_\") for c in DF_adult.columns]\n",
    "DF_adult['income_label'] = (DF_adult[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "DF_adult.drop(\"income\", axis=1, inplace=True)\n",
    "DF_adult['age_buckets'] = pd.cut(DF_adult.age, bins=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65],\n",
    "    labels=np.arange(9))\n",
    "out_dir = DATA_PATH/'adult/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "wide_cols = ['age_buckets', 'education', 'relationship','workclass','occupation',\n",
    "    'native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "# DEEP DENSE\n",
    "embeddings_cols = [('education',16), ('relationship',16), ('workclass',16),\n",
    "    ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "standardize_cols = continuous_cols\n",
    "\n",
    "#TARGET: logistic\n",
    "target = 'income_label'\n",
    "DF_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide and Deep adult data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_adult = prepare_data_adult(\n",
    "    DF_adult, wide_cols,\n",
    "    crossed_cols,\n",
    "    embeddings_cols,\n",
    "    continuous_cols,\n",
    "    standardize_cols,\n",
    "    target, out_dir,\n",
    "    scale=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'cat_embeddings_input', 'cat_embeddings_encoding_dict', 'continuous_cols', 'deep_column_idx'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset_adult.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wide', 'deep_dense', 'target'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset_adult['train'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_adult['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_adult['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_adult['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_adult['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_adult['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "model1 = WideDeep(output_dim=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=805, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_education): Embedding(16, 16)\n",
       "    (emb_layer_relationship): Embedding(6, 16)\n",
       "    (emb_layer_workclass): Embedding(9, 16)\n",
       "    (emb_layer_occupation): Embedding(15, 16)\n",
       "    (emb_layer_native_country): Embedding(42, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=82, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compile and Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer={'widedeep': ['Adam', 0.01]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(method='logistic', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 229/229 [00:02<00:00, 114.23it/s, acc=0.83, loss=0.367] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 167.71it/s, acc=0.84, loss=0.356] \n",
      "epoch 2: 100%|██████████| 229/229 [00:01<00:00, 127.67it/s, acc=0.84, loss=0.346] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.71it/s, acc=0.841, loss=0.345]\n",
      "epoch 3: 100%|██████████| 229/229 [00:01<00:00, 128.27it/s, acc=0.842, loss=0.342]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 172.08it/s, acc=0.838, loss=0.348]\n",
      "epoch 4: 100%|██████████| 229/229 [00:01<00:00, 128.06it/s, acc=0.842, loss=0.34] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.86it/s, acc=0.84, loss=0.349] \n",
      "epoch 5: 100%|██████████| 229/229 [00:01<00:00, 127.87it/s, acc=0.841, loss=0.339]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.27it/s, acc=0.841, loss=0.351]\n",
      "epoch 6: 100%|██████████| 229/229 [00:01<00:00, 127.71it/s, acc=0.844, loss=0.337]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.53it/s, acc=0.84, loss=0.349] \n",
      "epoch 7: 100%|██████████| 229/229 [00:01<00:00, 128.25it/s, acc=0.843, loss=0.336]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.37it/s, acc=0.841, loss=0.348]\n",
      "epoch 8: 100%|██████████| 229/229 [00:01<00:00, 128.31it/s, acc=0.843, loss=0.336]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.34it/s, acc=0.842, loss=0.348]\n",
      "epoch 9: 100%|██████████| 229/229 [00:01<00:00, 127.86it/s, acc=0.844, loss=0.335]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.76it/s, acc=0.84, loss=0.352] \n",
      "epoch 10: 100%|██████████| 229/229 [00:01<00:00, 128.19it/s, acc=0.843, loss=0.335]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.48it/s, acc=0.841, loss=0.352]\n"
     ]
    }
   ],
   "source": [
    "train_set = WideDeepLoader(wd_dataset_adult['train'], mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_adult['valid'], mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_adult['test'], mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)\n",
    "model1.fit(n_epochs=10, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression with the airbnb dataset using all: Wide, Deep_Dense, Deep_Text and Deep_Image. Also, multiple optimizers and learning rate schedulers\n",
    "\n",
    "### 2.1. Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assume you have runned airbnb_data_preprocessing.py and the resulting file is at \n",
    "# DATA_PATH/'airbnb/listings_processed.csv'\n",
    "DF_airbnb = pd.read_csv(DATA_PATH/'airbnb/listings_processed.csv')\n",
    "DF_airbnb = DF_airbnb[DF_airbnb.description.apply(lambda x: len(x.split(' '))>=10)]\n",
    "out_dir = DATA_PATH/'airbnb/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "crossed_cols = (['property_type', 'room_type'],)\n",
    "already_dummies = [c for c in DF_airbnb.columns if 'amenity' in c] + ['has_house_rules']\n",
    "wide_cols = ['is_location_exact', 'property_type', 'room_type', 'host_gender'] +\\\n",
    "    already_dummies\n",
    "\n",
    "#DEEP_DENSE\n",
    "embeddings_cols = [(c, 16) for c in DF_airbnb.columns if 'catg' in c] + [('neighbourhood_cleansed', 64)]\n",
    "continuous_cols = ['latitude', 'longitude', 'security_deposit', 'extra_people']\n",
    "standardize_cols = ['security_deposit', 'extra_people']\n",
    "\n",
    "# DEEP_TEXT\n",
    "text_col = 'description'\n",
    "word_vectors_path = 'data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# DEEP_IMAGE\n",
    "img_id = 'id'\n",
    "img_path = DATA_PATH/'airbnb/property_picture'\n",
    "\n",
    "#TARGET\n",
    "target = 'yield'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset, easy with `prepare_data_airbnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images from data/airbnb/property_picture\n",
      "BGR to RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/4999 [00:00<00:12, 406.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4999/4999 [00:12<00:00, 389.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 12434 words\n",
      "Indexing word vectors...\n",
      "Loaded 400000 word vectors\n",
      "Preparing embeddings matrix...\n",
      "6827 words in our vocabulary had glove vectors and appear more than the min frequency\n",
      "Wide and Deep airbnb data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_airbnb = prepare_data_airbnb(\n",
    "    # let's use only 5000 observations (not all of them will have images, so we might end with 4900+)\n",
    "    df = DF_airbnb.sample(5000),\n",
    "    img_id = img_id,\n",
    "    img_path = img_path,\n",
    "    text_col = text_col,\n",
    "    max_vocab = 20000,\n",
    "    min_freq = 2,\n",
    "    maxlen = 170,\n",
    "    word_vectors_path = word_vectors_path,\n",
    "    embeddings_cols = embeddings_cols,\n",
    "    continuous_cols = continuous_cols,\n",
    "    standardize_cols = standardize_cols,\n",
    "    target = target,\n",
    "    wide_cols = wide_cols,\n",
    "    crossed_cols = crossed_cols,\n",
    "    already_dummies = already_dummies,\n",
    "    out_dir = out_dir,\n",
    "    scale=True,\n",
    "    seed=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what all these parameters mean, simple see demo1 and demo2 and the modules in widedeep.models\n",
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_airbnb['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_airbnb['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_airbnb['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_airbnb['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_airbnb['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )\n",
    "params['deep_text'] = dict(\n",
    "    vocab_size = len(wd_dataset_airbnb['vocab'].itos),\n",
    "    embedding_dim = wd_dataset_airbnb['word_embeddings_matrix'].shape[1],\n",
    "    hidden_dim = 64,\n",
    "    n_layers = 3,\n",
    "    rnn_dropout = 0.5,\n",
    "    spatial_dropout = 0.1,\n",
    "    padding_idx = 1,\n",
    "    attention = False,\n",
    "    bidirectional = False,\n",
    "    embedding_matrix = wd_dataset_airbnb['word_embeddings_matrix']\n",
    "    )\n",
    "params['deep_img'] = dict(\n",
    "    pretrained = True,\n",
    "    freeze=6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build the Model\n",
    "\n",
    "The model is built exactly as in the case of the adult dataset before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "model2 = WideDeep(output_dim=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=213, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_neighbourhood_cleansed): Embedding(33, 64)\n",
       "    (emb_layer_beds_catg): Embedding(4, 16)\n",
       "    (emb_layer_host_listings_count_catg): Embedding(4, 16)\n",
       "    (emb_layer_bathrooms_catg): Embedding(3, 16)\n",
       "    (emb_layer_minimum_nights_catg): Embedding(3, 16)\n",
       "    (emb_layer_bedrooms_catg): Embedding(4, 16)\n",
       "    (emb_layer_accommodates_catg): Embedding(3, 16)\n",
       "    (emb_layer_guests_included_catg): Embedding(3, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (deep_text): DeepText(\n",
       "    (embedding_dropout): Dropout2d(p=0.1)\n",
       "    (embedding): Embedding(7165, 300, padding_idx=1)\n",
       "    (rnn): GRU(300, 64, num_layers=3, batch_first=True, dropout=0.5)\n",
       "    (dtlinear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_img): DeepImage(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (dilinear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compile and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example...\n",
    "optimizer=dict(\n",
    "    wide=['Adam', 0.1],\n",
    "    deep_dense=['Adam', 0.01],\n",
    "    deep_text=['RMSprop', 0.01,0.1],\n",
    "    deep_img= ['Adam', 0.01]\n",
    "    )\n",
    "lr_scheduler=dict(\n",
    "    wide=['StepLR', 3, 0.1],\n",
    "    deep_dense=['StepLR', 3, 0.1],\n",
    "    deep_text=['MultiStepLR', [3,5,7], 0.1],\n",
    "    deep_img=['MultiStepLR', [3,5,7], 0.1]\n",
    "    )\n",
    "# if you want just one optimizer and lr_scheduler call simply\n",
    "# optimizer={'widedeep': ['Adam', 0.01]}\n",
    "# lr_scheduler = {'widedeep': ['StepLR', 3, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(method='regression', optimizer=optimizer, lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406] #RGB\n",
    "std=[0.229, 0.224, 0.225] # RGB\n",
    "transform  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "train_set = WideDeepLoader(wd_dataset_airbnb['train'], transform, mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_airbnb['valid'], transform, mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_airbnb['test'], transform, mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 24/24 [00:21<00:00,  1.24it/s, loss=128]\n",
      "valid: 100%|██████████| 8/8 [00:03<00:00,  2.11it/s, loss=1.3e+3] \n",
      "epoch 2: 100%|██████████| 24/24 [00:21<00:00,  1.25it/s, loss=110]\n",
      "valid: 100%|██████████| 8/8 [00:03<00:00,  2.07it/s, loss=98.8]\n",
      "epoch 3: 100%|██████████| 24/24 [00:21<00:00,  1.25it/s, loss=106]\n",
      "valid: 100%|██████████| 8/8 [00:03<00:00,  2.08it/s, loss=98.3]\n",
      "epoch 4: 100%|██████████| 24/24 [00:21<00:00,  1.25it/s, loss=106]\n",
      "valid: 100%|██████████| 8/8 [00:04<00:00,  2.03it/s, loss=97.6]\n",
      "epoch 5: 100%|██████████| 24/24 [00:21<00:00,  1.26it/s, loss=104]\n",
      "valid: 100%|██████████| 8/8 [00:04<00:00,  2.05it/s, loss=98.2]\n"
     ]
    }
   ],
   "source": [
    "model2.fit(n_epochs=5, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 32/32 [00:04<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.62761727684484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(test_loader)\n",
    "y = wd_dataset_airbnb['test']['target']\n",
    "print(np.sqrt(mean_squared_error(y, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Extract the learned embeddings for a given categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_cleansed_emb = model2.get_embeddings(col_name='neighbourhood_cleansed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hackney': array([-1.974714,  0.863323, -1.459528,  1.365437, ..., -0.14018 , -0.524041, -0.316058, -0.410313], dtype=float32),\n",
       " 'Camden': array([ 0.410322, -0.724736, -0.56029 ,  0.079272, ..., -0.220795, -1.234073, -2.003421,  0.949352], dtype=float32),\n",
       " 'Hillingdon': array([-0.853097,  0.266621,  0.08445 ,  1.554609, ..., -0.139359, -0.419641,  0.74903 , -0.73454 ], dtype=float32),\n",
       " 'Southwark': array([-0.025719,  0.1034  , -0.930781,  0.259913, ..., -0.762355,  0.91882 , -0.158588, -0.399257], dtype=float32),\n",
       " 'Islington': array([-1.344581,  0.027291, -0.482429, -0.05447 , ...,  0.00804 ,  1.058191,  1.389844,  0.976658], dtype=float32),\n",
       " 'Lambeth': array([-0.503692,  1.997271,  0.204253,  2.161032, ..., -1.5392  , -0.274035,  1.052611, -1.117137], dtype=float32),\n",
       " 'Westminster': array([-0.160366, -0.897001, -1.555466, -0.108475, ...,  0.095278, -1.265424,  0.48901 ,  1.249886], dtype=float32),\n",
       " 'Kensington and Chelsea': array([-0.941537, -0.79041 , -0.163071,  2.514469, ..., -1.033578, -2.369078,  0.659031, -1.707059], dtype=float32),\n",
       " 'Ealing': array([-0.175527,  0.796092, -0.495381, -0.922058, ...,  2.760556,  1.751993,  0.105334, -0.786012], dtype=float32),\n",
       " 'Newham': array([-1.281402,  0.662634,  1.103911, -0.32145 , ..., -0.99564 , -0.359498, -0.36606 , -0.431747], dtype=float32),\n",
       " 'Tower Hamlets': array([ 0.271887, -0.468121,  0.431422,  0.281864, ...,  1.437819, -0.144674, -0.62363 ,  1.09578 ], dtype=float32),\n",
       " 'Waltham Forest': array([ 0.666032, -0.498509,  1.038276, -0.642088, ...,  1.354977, -0.626648,  0.817839, -0.277589], dtype=float32),\n",
       " 'Wandsworth': array([ 0.07317 ,  1.094801,  1.50066 , -1.284966, ..., -0.905864, -0.143956,  0.490856, -1.177169], dtype=float32),\n",
       " 'Bromley': array([-0.486731,  1.064391, -0.700393,  1.08445 , ...,  1.852425, -2.306012,  0.341955,  1.354509], dtype=float32),\n",
       " 'Merton': array([ 0.910861,  0.590595,  1.780486,  0.106199, ..., -0.178996,  1.284232, -1.418589, -0.559087], dtype=float32),\n",
       " 'Harrow': array([-0.586278, -1.161813, -0.196221, -1.276113, ..., -0.647752, -0.788815,  0.564165, -1.408766], dtype=float32),\n",
       " 'Hammersmith and Fulham': array([ 0.126797,  1.058752, -0.480162, -0.246861, ..., -1.77949 , -0.367484, -0.493584,  1.963025], dtype=float32),\n",
       " 'Croydon': array([ 0.831145,  1.084541,  0.18434 , -1.17936 , ..., -0.644541,  1.287308, -0.437237, -0.036385], dtype=float32),\n",
       " 'Kingston upon Thames': array([-1.138847,  0.353119,  0.453814, -1.537953, ...,  0.433265, -1.492994,  0.49617 ,  0.32435 ], dtype=float32),\n",
       " 'Greenwich': array([-0.375921, -0.006278, -1.202842,  0.710625, ..., -1.025979,  1.733119,  0.30344 , -0.333253], dtype=float32),\n",
       " 'Barnet': array([ 0.112118,  0.253998, -0.083625,  0.991614, ..., -0.534774, -0.836971, -0.546797, -0.074188], dtype=float32),\n",
       " 'Sutton': array([-0.581444,  1.245627, -0.268801,  0.220981, ..., -0.484765,  0.235929, -0.339714,  0.383085], dtype=float32),\n",
       " 'Brent': array([ 0.684018,  1.122321, -0.578836,  1.606611, ...,  0.772141,  1.470037,  0.896998, -0.527985], dtype=float32),\n",
       " 'Hounslow': array([ 0.186708, -0.859478,  1.421476, -0.367777, ...,  0.641241, -1.193496, -0.028203, -0.5517  ], dtype=float32),\n",
       " 'Haringey': array([-0.520568,  1.791861, -1.58973 , -0.191609, ..., -0.763657,  0.877533, -0.751659, -0.779809], dtype=float32),\n",
       " 'Lewisham': array([ 0.94821 ,  1.358798,  2.511026, -0.085026, ..., -0.201893,  0.56487 , -1.297939, -0.988157], dtype=float32),\n",
       " 'Enfield': array([ 0.839919,  3.486744,  0.209266,  0.486838, ..., -0.255266,  1.530212,  0.036456, -1.841724], dtype=float32),\n",
       " 'Redbridge': array([ 0.562378,  0.117497, -0.163867, -0.418552, ..., -1.083247,  0.736799,  0.942412, -1.175167], dtype=float32),\n",
       " 'Richmond upon Thames': array([-0.454952,  1.581438,  2.934833,  0.976836, ..., -1.138685, -0.491665, -1.153073,  0.511827], dtype=float32),\n",
       " 'Barking and Dagenham': array([-1.24154 , -0.185623,  0.239697,  1.636263, ...,  1.715875, -1.810889,  0.612478,  0.533097], dtype=float32),\n",
       " 'Bexley': array([ 0.593676,  1.990228,  0.449346, -1.309624, ...,  0.541702,  1.06037 , -0.971935, -1.45975 ], dtype=float32),\n",
       " 'City of London': array([ 0.210624,  0.413433, -0.161755,  0.762585, ...,  2.038467,  0.73799 ,  0.154868, -0.416979], dtype=float32),\n",
       " 'Havering': array([ 0.082899, -0.406886,  0.492994,  0.116197, ...,  0.465383, -1.56611 , -1.431871, -0.940736], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbourhood_cleansed_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiclass Classification with the airbnb dataset using Wide, Deep_Dense and Deep_Text with one optimizers and one learning rate scheduler\n",
    "\n",
    "Here we will fake a multiclass classification problem using the target `yield`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_airbnb['yield_cat'] = pd.cut(DF_airbnb['yield'], bins=[0.2, 65, 163, 600], labels=[0,1,2])\n",
    "DF_airbnb.drop('yield', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is the same\n",
    "out_dir = DATA_PATH/'airbnb/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "crossed_cols = (['property_type', 'room_type'],)\n",
    "already_dummies = [c for c in DF_airbnb.columns if 'amenity' in c] + ['has_house_rules']\n",
    "wide_cols = ['is_location_exact', 'property_type', 'room_type', 'host_gender'] +\\\n",
    "    already_dummies\n",
    "\n",
    "#DEEP_DENSE\n",
    "embeddings_cols = [(c, 16) for c in DF_airbnb.columns if 'catg' in c] + [('neighbourhood_cleansed', 64)]\n",
    "continuous_cols = ['latitude', 'longitude', 'security_deposit', 'extra_people']\n",
    "standardize_cols = ['security_deposit', 'extra_people']\n",
    "\n",
    "# DEEP_TEXT\n",
    "text_col = 'description'\n",
    "word_vectors_path = 'data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# DEEP_IMAGE\n",
    "img_id = 'id'\n",
    "img_path = DATA_PATH/'airbnb/property_picture'\n",
    "\n",
    "#TARGET\n",
    "target = 'yield_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images from data/airbnb/property_picture\n",
      "BGR to RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/4998 [00:00<00:12, 402.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4998/4998 [00:12<00:00, 392.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 12433 words\n",
      "Indexing word vectors...\n",
      "Loaded 400000 word vectors\n",
      "Preparing embeddings matrix...\n",
      "6869 words in our vocabulary had glove vectors and appear more than the min frequency\n",
      "Wide and Deep airbnb data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_airbnb = prepare_data_airbnb(\n",
    "    # let's use only 5000 observations (not all of them will have images, so we might end with 4900+)\n",
    "    df = DF_airbnb.sample(5000),\n",
    "    img_id = img_id,\n",
    "    img_path = img_path,\n",
    "    text_col = text_col,\n",
    "    max_vocab = 20000,\n",
    "    min_freq = 2,\n",
    "    maxlen = 170,\n",
    "    word_vectors_path = word_vectors_path,\n",
    "    embeddings_cols = embeddings_cols,\n",
    "    continuous_cols = continuous_cols,\n",
    "    standardize_cols = standardize_cols,\n",
    "    target = target,\n",
    "    wide_cols = wide_cols,\n",
    "    crossed_cols = crossed_cols,\n",
    "    already_dummies = already_dummies,\n",
    "    out_dir = out_dir,\n",
    "    scale=True,\n",
    "    seed=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[115, 153, 158],\n",
       "         [139, 182, 189],\n",
       "         [152, 195, 201],\n",
       "         [157, 205, 199],\n",
       "         ...,\n",
       "         [128,  87,  61],\n",
       "         [124,  81,  54],\n",
       "         [132,  85,  50],\n",
       "         [140,  93,  58]],\n",
       "\n",
       "        [[143, 179, 182],\n",
       "         [149, 189, 193],\n",
       "         [149, 189, 194],\n",
       "         [149, 194, 190],\n",
       "         ...,\n",
       "         [180, 127,  91],\n",
       "         [166, 114,  78],\n",
       "         [163, 113,  72],\n",
       "         [161, 111,  71]],\n",
       "\n",
       "        [[138, 170, 169],\n",
       "         [165, 201, 201],\n",
       "         [142, 178, 179],\n",
       "         [147, 186, 187],\n",
       "         ...,\n",
       "         [168, 104,  57],\n",
       "         [148,  87,  42],\n",
       "         [141,  86,  38],\n",
       "         [138,  86,  41]],\n",
       "\n",
       "        [[145, 174, 170],\n",
       "         [170, 202, 198],\n",
       "         [143, 175, 173],\n",
       "         [141, 174, 177],\n",
       "         ...,\n",
       "         [140,  75,  26],\n",
       "         [133,  72,  24],\n",
       "         [137,  81,  29],\n",
       "         [135,  83,  34]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 82,  73,  71],\n",
       "         [ 83,  77,  74],\n",
       "         [ 81,  75,  73],\n",
       "         [ 64,  50,  46],\n",
       "         ...,\n",
       "         [ 76,  67,  68],\n",
       "         [ 74,  66,  67],\n",
       "         [ 74,  68,  68],\n",
       "         [ 74,  65,  66]],\n",
       "\n",
       "        [[ 84,  75,  73],\n",
       "         [ 86,  80,  77],\n",
       "         [ 74,  67,  66],\n",
       "         [ 58,  46,  43],\n",
       "         ...,\n",
       "         [ 76,  67,  68],\n",
       "         [ 73,  65,  66],\n",
       "         [ 73,  67,  67],\n",
       "         [ 74,  66,  67]],\n",
       "\n",
       "        [[ 85,  77,  74],\n",
       "         [ 85,  79,  76],\n",
       "         [ 64,  58,  56],\n",
       "         [ 58,  47,  46],\n",
       "         ...,\n",
       "         [ 77,  68,  69],\n",
       "         [ 76,  68,  69],\n",
       "         [ 73,  67,  67],\n",
       "         [ 72,  64,  64]],\n",
       "\n",
       "        [[ 85,  76,  74],\n",
       "         [ 81,  75,  72],\n",
       "         [ 66,  60,  59],\n",
       "         [ 69,  60,  60],\n",
       "         ...,\n",
       "         [ 78,  69,  70],\n",
       "         [ 77,  69,  70],\n",
       "         [ 74,  68,  68],\n",
       "         [ 75,  66,  67]]],\n",
       "\n",
       "\n",
       "       [[[197, 168, 128],\n",
       "         [196, 168, 128],\n",
       "         [200, 173, 136],\n",
       "         [209, 187, 154],\n",
       "         ...,\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [179, 149, 111]],\n",
       "\n",
       "        [[208, 179, 137],\n",
       "         [203, 174, 134],\n",
       "         [196, 167, 129],\n",
       "         [193, 168, 132],\n",
       "         ...,\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [179, 149, 111]],\n",
       "\n",
       "        [[210, 179, 134],\n",
       "         [210, 180, 138],\n",
       "         [210, 180, 140],\n",
       "         [206, 177, 136],\n",
       "         ...,\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [179, 149, 111]],\n",
       "\n",
       "        [[211, 181, 132],\n",
       "         [212, 181, 134],\n",
       "         [212, 180, 135],\n",
       "         [215, 183, 136],\n",
       "         ...,\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [180, 150, 112],\n",
       "         [179, 149, 111]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[183, 209, 244],\n",
       "         [181, 207, 242],\n",
       "         [180, 205, 240],\n",
       "         [179, 202, 236],\n",
       "         ...,\n",
       "         [175, 112,  58],\n",
       "         [174, 111,  57],\n",
       "         [173, 110,  56],\n",
       "         [172, 109,  55]],\n",
       "\n",
       "        [[180, 205, 240],\n",
       "         [177, 202, 237],\n",
       "         [173, 198, 232],\n",
       "         [169, 192, 226],\n",
       "         ...,\n",
       "         [174, 111,  57],\n",
       "         [174, 111,  57],\n",
       "         [175, 112,  58],\n",
       "         [174, 111,  57]],\n",
       "\n",
       "        [[176, 199, 233],\n",
       "         [172, 195, 229],\n",
       "         [168, 191, 225],\n",
       "         [168, 191, 223],\n",
       "         ...,\n",
       "         [175, 112,  58],\n",
       "         [174, 111,  57],\n",
       "         [173, 110,  56],\n",
       "         [172, 109,  55]],\n",
       "\n",
       "        [[170, 191, 226],\n",
       "         [167, 188, 222],\n",
       "         [169, 189, 222],\n",
       "         [169, 192, 224],\n",
       "         ...,\n",
       "         [177, 114,  60],\n",
       "         [175, 112,  58],\n",
       "         [174, 111,  57],\n",
       "         [172, 109,  55]]],\n",
       "\n",
       "\n",
       "       [[[197, 201, 204],\n",
       "         [198, 202, 205],\n",
       "         [198, 202, 205],\n",
       "         [201, 202, 206],\n",
       "         ...,\n",
       "         [189, 190, 192],\n",
       "         [188, 189, 191],\n",
       "         [187, 188, 190],\n",
       "         [186, 187, 189]],\n",
       "\n",
       "        [[196, 200, 203],\n",
       "         [197, 201, 204],\n",
       "         [198, 202, 205],\n",
       "         [201, 202, 206],\n",
       "         ...,\n",
       "         [189, 190, 192],\n",
       "         [189, 190, 192],\n",
       "         [188, 189, 191],\n",
       "         [187, 188, 190]],\n",
       "\n",
       "        [[196, 199, 202],\n",
       "         [197, 200, 203],\n",
       "         [198, 200, 204],\n",
       "         [200, 201, 205],\n",
       "         ...,\n",
       "         [190, 191, 193],\n",
       "         [190, 191, 193],\n",
       "         [189, 190, 192],\n",
       "         [188, 189, 191]],\n",
       "\n",
       "        [[197, 198, 202],\n",
       "         [198, 199, 203],\n",
       "         [198, 199, 203],\n",
       "         [199, 200, 204],\n",
       "         ...,\n",
       "         [191, 192, 194],\n",
       "         [190, 191, 193],\n",
       "         [189, 190, 192],\n",
       "         [189, 190, 192]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[152, 143,  98],\n",
       "         [152, 143, 100],\n",
       "         [158, 145, 103],\n",
       "         [150, 137,  93],\n",
       "         ...,\n",
       "         [179, 178, 180],\n",
       "         [175, 172, 171],\n",
       "         [175, 165, 161],\n",
       "         [171, 161, 155]],\n",
       "\n",
       "        [[151, 142,  97],\n",
       "         [150, 141,  98],\n",
       "         [156, 144, 102],\n",
       "         [157, 147, 107],\n",
       "         ...,\n",
       "         [195, 206, 217],\n",
       "         [188, 195, 206],\n",
       "         [186, 186, 195],\n",
       "         [203, 201, 208]],\n",
       "\n",
       "        [[158, 149, 103],\n",
       "         [159, 151, 105],\n",
       "         [171, 159, 116],\n",
       "         [167, 159, 120],\n",
       "         ...,\n",
       "         [188, 197, 212],\n",
       "         [184, 191, 207],\n",
       "         [175, 178, 196],\n",
       "         [187, 189, 205]],\n",
       "\n",
       "        [[170, 161, 115],\n",
       "         [172, 164, 118],\n",
       "         [175, 163, 119],\n",
       "         [174, 163, 122],\n",
       "         ...,\n",
       "         [188, 194, 209],\n",
       "         [187, 192, 208],\n",
       "         [183, 185, 203],\n",
       "         [174, 177, 194]]],\n",
       "\n",
       "\n",
       "       [[[ 87,  95, 107],\n",
       "         [ 84,  92, 103],\n",
       "         [ 83,  90, 101],\n",
       "         [ 80,  87,  93],\n",
       "         ...,\n",
       "         [106,  96,  87],\n",
       "         [106,  96,  87],\n",
       "         [106,  97,  88],\n",
       "         [105,  96,  87]],\n",
       "\n",
       "        [[ 89,  97, 110],\n",
       "         [ 86,  94, 106],\n",
       "         [ 84,  92, 103],\n",
       "         [ 82,  89,  95],\n",
       "         ...,\n",
       "         [106,  96,  87],\n",
       "         [106,  96,  87],\n",
       "         [106,  97,  88],\n",
       "         [105,  96,  87]],\n",
       "\n",
       "        [[ 92, 101, 114],\n",
       "         [ 89,  97, 110],\n",
       "         [ 87,  94, 107],\n",
       "         [ 87,  92,  98],\n",
       "         ...,\n",
       "         [106,  96,  87],\n",
       "         [106,  96,  87],\n",
       "         [106,  97,  88],\n",
       "         [105,  96,  87]],\n",
       "\n",
       "        [[ 96, 103, 120],\n",
       "         [ 91,  99, 115],\n",
       "         [ 89,  96, 110],\n",
       "         [ 90,  95, 101],\n",
       "         ...,\n",
       "         [106,  96,  87],\n",
       "         [106,  96,  87],\n",
       "         [106,  97,  88],\n",
       "         [105,  96,  87]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         ...,\n",
       "         [ 53,  39,  38],\n",
       "         [ 51,  37,  36],\n",
       "         [ 50,  36,  35],\n",
       "         [ 51,  37,  36]],\n",
       "\n",
       "        [[ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         ...,\n",
       "         [ 49,  35,  34],\n",
       "         [ 47,  33,  32],\n",
       "         [ 45,  31,  30],\n",
       "         [ 49,  35,  34]],\n",
       "\n",
       "        [[ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         [ 65,  62,  57],\n",
       "         ...,\n",
       "         [ 41,  27,  26],\n",
       "         [ 43,  29,  28],\n",
       "         [ 46,  32,  31],\n",
       "         [ 46,  32,  31]],\n",
       "\n",
       "        [[ 66,  63,  58],\n",
       "         [ 65,  62,  57],\n",
       "         [ 64,  61,  56],\n",
       "         [ 65,  62,  57],\n",
       "         ...,\n",
       "         [ 35,  21,  20],\n",
       "         [ 44,  30,  29],\n",
       "         [ 50,  36,  35],\n",
       "         [ 44,  30,  29]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [182, 181, 177],\n",
       "         ...,\n",
       "         [198, 197, 192],\n",
       "         [195, 194, 189],\n",
       "         [190, 189, 184],\n",
       "         [195, 194, 189]],\n",
       "\n",
       "        [[180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [182, 181, 177],\n",
       "         ...,\n",
       "         [198, 197, 192],\n",
       "         [193, 192, 187],\n",
       "         [190, 189, 184],\n",
       "         [195, 194, 189]],\n",
       "\n",
       "        [[180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [182, 181, 177],\n",
       "         ...,\n",
       "         [197, 196, 191],\n",
       "         [192, 191, 186],\n",
       "         [190, 189, 184],\n",
       "         [195, 194, 189]],\n",
       "\n",
       "        [[180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [180, 179, 175],\n",
       "         [182, 181, 177],\n",
       "         ...,\n",
       "         [196, 195, 190],\n",
       "         [193, 192, 187],\n",
       "         [190, 189, 184],\n",
       "         [195, 194, 189]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 55,  47,  44],\n",
       "         [ 83,  75,  72],\n",
       "         [100,  92,  89],\n",
       "         [ 72,  64,  62],\n",
       "         ...,\n",
       "         [ 76,  36,  44],\n",
       "         [ 77,  34,  41],\n",
       "         [ 79,  33,  38],\n",
       "         [ 85,  44,  48]],\n",
       "\n",
       "        [[ 74,  64,  62],\n",
       "         [ 65,  55,  53],\n",
       "         [ 73,  64,  62],\n",
       "         [ 86,  78,  76],\n",
       "         ...,\n",
       "         [ 77,  37,  45],\n",
       "         [ 89,  47,  55],\n",
       "         [ 79,  36,  44],\n",
       "         [ 71,  33,  39]],\n",
       "\n",
       "        [[ 61,  51,  49],\n",
       "         [ 80,  70,  68],\n",
       "         [ 67,  57,  55],\n",
       "         [ 64,  56,  54],\n",
       "         ...,\n",
       "         [ 85,  45,  53],\n",
       "         [ 86,  46,  54],\n",
       "         [ 79,  39,  48],\n",
       "         [ 61,  27,  33]],\n",
       "\n",
       "        [[ 81,  71,  69],\n",
       "         [ 73,  63,  61],\n",
       "         [ 63,  53,  51],\n",
       "         [ 62,  54,  52],\n",
       "         ...,\n",
       "         [ 77,  37,  45],\n",
       "         [ 78,  38,  46],\n",
       "         [ 85,  48,  55],\n",
       "         [ 46,  14,  19]]],\n",
       "\n",
       "\n",
       "       [[[224, 224, 232],\n",
       "         [224, 224, 232],\n",
       "         [224, 224, 232],\n",
       "         [225, 225, 235],\n",
       "         ...,\n",
       "         [215, 208, 215],\n",
       "         [216, 208, 215],\n",
       "         [217, 207, 215],\n",
       "         [217, 207, 215]],\n",
       "\n",
       "        [[225, 225, 233],\n",
       "         [224, 224, 232],\n",
       "         [223, 223, 231],\n",
       "         [223, 223, 233],\n",
       "         ...,\n",
       "         [215, 208, 215],\n",
       "         [215, 207, 215],\n",
       "         [217, 207, 215],\n",
       "         [217, 207, 215]],\n",
       "\n",
       "        [[227, 227, 235],\n",
       "         [225, 225, 233],\n",
       "         [224, 224, 232],\n",
       "         [223, 223, 233],\n",
       "         ...,\n",
       "         [214, 207, 214],\n",
       "         [215, 206, 214],\n",
       "         [216, 206, 214],\n",
       "         [216, 206, 214]],\n",
       "\n",
       "        [[228, 228, 236],\n",
       "         [228, 228, 236],\n",
       "         [228, 228, 236],\n",
       "         [227, 227, 237],\n",
       "         ...,\n",
       "         [213, 206, 213],\n",
       "         [214, 206, 213],\n",
       "         [215, 205, 213],\n",
       "         [215, 205, 213]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[120, 114, 121],\n",
       "         [123, 116, 123],\n",
       "         [125, 118, 125],\n",
       "         [124, 117, 124],\n",
       "         ...,\n",
       "         [ 83,  48,  15],\n",
       "         [ 76,  44,  15],\n",
       "         [ 57,  32,  14],\n",
       "         [127, 110, 106]],\n",
       "\n",
       "        [[125, 119, 125],\n",
       "         [124, 117, 124],\n",
       "         [125, 118, 125],\n",
       "         [129, 122, 129],\n",
       "         ...,\n",
       "         [ 80,  44,   8],\n",
       "         [ 74,  42,  10],\n",
       "         [ 58,  32,  13],\n",
       "         [128, 109, 105]],\n",
       "\n",
       "        [[130, 124, 130],\n",
       "         [129, 122, 129],\n",
       "         [131, 124, 131],\n",
       "         [131, 124, 131],\n",
       "         ...,\n",
       "         [ 81,  46,   7],\n",
       "         [ 77,  43,  10],\n",
       "         [ 59,  32,  13],\n",
       "         [130, 109, 105]],\n",
       "\n",
       "        [[145, 139, 145],\n",
       "         [132, 125, 132],\n",
       "         [132, 125, 132],\n",
       "         [129, 122, 129],\n",
       "         ...,\n",
       "         [ 95,  58,  17],\n",
       "         [ 85,  50,  15],\n",
       "         [ 66,  35,  15],\n",
       "         [133, 110, 106]]],\n",
       "\n",
       "\n",
       "       [[[214, 203, 197],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         ...,\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 182],\n",
       "         [199, 186, 180]],\n",
       "\n",
       "        [[214, 203, 197],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         ...,\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 182],\n",
       "         [199, 186, 180]],\n",
       "\n",
       "        [[214, 203, 197],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         ...,\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 182],\n",
       "         [199, 186, 180]],\n",
       "\n",
       "        [[214, 203, 197],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         [216, 205, 199],\n",
       "         ...,\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 183],\n",
       "         [200, 186, 182],\n",
       "         [199, 186, 180]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[198, 153, 111],\n",
       "         [196, 151, 109],\n",
       "         [199, 152, 110],\n",
       "         [199, 152, 108],\n",
       "         ...,\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 227, 234]],\n",
       "\n",
       "        [[199, 154, 112],\n",
       "         [196, 151, 108],\n",
       "         [200, 153, 109],\n",
       "         [197, 150, 106],\n",
       "         ...,\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 227, 234]],\n",
       "\n",
       "        [[199, 154, 111],\n",
       "         [197, 152, 108],\n",
       "         [198, 151, 107],\n",
       "         [191, 144,  99],\n",
       "         ...,\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 227, 234]],\n",
       "\n",
       "        [[197, 153, 108],\n",
       "         [197, 152, 107],\n",
       "         [197, 150, 105],\n",
       "         [196, 149, 103],\n",
       "         ...,\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 229, 235],\n",
       "         [224, 227, 234]]],\n",
       "\n",
       "\n",
       "       [[[232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [231, 233, 230],\n",
       "         ...,\n",
       "         [164, 158, 142],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141]],\n",
       "\n",
       "        [[232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [231, 233, 230],\n",
       "         ...,\n",
       "         [164, 158, 142],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141]],\n",
       "\n",
       "        [[232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [231, 233, 230],\n",
       "         ...,\n",
       "         [164, 158, 142],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141]],\n",
       "\n",
       "        [[232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [232, 234, 233],\n",
       "         [231, 233, 230],\n",
       "         ...,\n",
       "         [164, 158, 142],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141],\n",
       "         [163, 157, 141]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 104,  99],\n",
       "         [117, 108, 103],\n",
       "         [116, 107, 101],\n",
       "         [109, 105,  94],\n",
       "         ...,\n",
       "         [124,  85,  53],\n",
       "         [121,  84,  51],\n",
       "         [111,  81,  46],\n",
       "         [ 94,  69,  41]],\n",
       "\n",
       "        [[114, 105, 100],\n",
       "         [114, 105, 100],\n",
       "         [110, 101,  96],\n",
       "         [105, 101,  92],\n",
       "         ...,\n",
       "         [119,  81,  43],\n",
       "         [114,  77,  39],\n",
       "         [111,  79,  42],\n",
       "         [ 94,  68,  38]],\n",
       "\n",
       "        [[114, 105, 100],\n",
       "         [110, 101,  96],\n",
       "         [107,  99,  93],\n",
       "         [167, 164, 158],\n",
       "         ...,\n",
       "         [125,  88,  44],\n",
       "         [116,  79,  36],\n",
       "         [115,  82,  43],\n",
       "         [ 99,  72,  39]],\n",
       "\n",
       "        [[111, 102,  97],\n",
       "         [107,  98,  93],\n",
       "         [165, 157, 152],\n",
       "         [213, 211, 207],\n",
       "         ...,\n",
       "         [128,  91,  46],\n",
       "         [130,  93,  48],\n",
       "         [124,  90,  48],\n",
       "         [111,  83,  48]]]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's drop the image dataset, this time \"only\" with Wide, Deep_Dense and Deep_Text\n",
    "wd_dataset_airbnb['train'].pop('deep_img')\n",
    "wd_dataset_airbnb['valid'].pop('deep_img')\n",
    "wd_dataset_airbnb['test'].pop('deep_img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_airbnb['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_airbnb['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_airbnb['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_airbnb['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_airbnb['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )\n",
    "params['deep_text'] = dict(\n",
    "    vocab_size = len(wd_dataset_airbnb['vocab'].itos),\n",
    "    embedding_dim = wd_dataset_airbnb['word_embeddings_matrix'].shape[1],\n",
    "    hidden_dim = 64,\n",
    "    n_layers = 3,\n",
    "    rnn_dropout = 0.5,\n",
    "    spatial_dropout = 0.1,\n",
    "    padding_idx = 1,\n",
    "    attention = False,\n",
    "    bidirectional = False,\n",
    "    embedding_matrix = wd_dataset_airbnb['word_embeddings_matrix']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "# We have 3 classes\n",
    "model3 = WideDeep(output_dim=3, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=213, out_features=3, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_neighbourhood_cleansed): Embedding(33, 64)\n",
       "    (emb_layer_bathrooms_catg): Embedding(3, 16)\n",
       "    (emb_layer_bedrooms_catg): Embedding(4, 16)\n",
       "    (emb_layer_guests_included_catg): Embedding(3, 16)\n",
       "    (emb_layer_beds_catg): Embedding(4, 16)\n",
       "    (emb_layer_minimum_nights_catg): Embedding(3, 16)\n",
       "    (emb_layer_host_listings_count_catg): Embedding(4, 16)\n",
       "    (emb_layer_accommodates_catg): Embedding(3, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (deep_text): DeepText(\n",
       "    (embedding_dropout): Dropout2d(p=0.1)\n",
       "    (embedding): Embedding(7208, 300, padding_idx=1)\n",
       "    (rnn): GRU(300, 64, num_layers=3, batch_first=True, dropout=0.5)\n",
       "    (dtlinear): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.4 Compile and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer={'widedeep': ['Adam', 0.01]}\n",
    "lr_scheduler = {'widedeep': ['StepLR', 3, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(method='multiclass', optimizer=optimizer, lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WideDeepLoader(wd_dataset_airbnb['train'], mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_airbnb['valid'], mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_airbnb['test'], mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wide', 'deep_dense', 'deep_text', 'target']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 24/24 [00:03<00:00,  7.01it/s, acc=0.512, loss=1.01]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 38.83it/s, acc=0.552, loss=0.986]\n",
      "epoch 2: 100%|██████████| 24/24 [00:03<00:00,  7.47it/s, acc=0.577, loss=0.96] \n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 40.44it/s, acc=0.557, loss=0.967]\n",
      "epoch 3: 100%|██████████| 24/24 [00:03<00:00,  7.74it/s, acc=0.61, loss=0.929] \n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 41.64it/s, acc=0.567, loss=0.961]\n",
      "epoch 4: 100%|██████████| 24/24 [00:03<00:00,  8.20it/s, acc=0.635, loss=0.911]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 42.00it/s, acc=0.562, loss=0.973]\n",
      "epoch 5: 100%|██████████| 24/24 [00:02<00:00,  8.62it/s, acc=0.646, loss=0.894]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 45.36it/s, acc=0.536, loss=0.981]\n"
     ]
    }
   ],
   "source": [
    "model3.fit(n_epochs=5, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont pay much attention to the results, this is just an artificial experiments for you to see how one would use it for multiclass classification. \n",
    "\n",
    "And with this, this is it, I guess you now have all the information to run as many experiments as you want combining all sorts of datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
