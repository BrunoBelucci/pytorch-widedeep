{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the model\n",
    "\n",
    "To understand the model it would be convenient if you have gone through demo1 and 2, however you can learn how to use the model simply reading this notebook.\n",
    "\n",
    "### 0. Load the data\n",
    "\n",
    "Note that, as long as your dataset is in a state similar to that of adult.csv or listings.csv (after `airbnb_data_preprocessing.py`), i.e. remove NaN, impute missing values, etc..., you are \"good to go\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prepare_data import prepare_data_adult, prepare_data_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Logistic regression with the adult dataset\n",
    "\n",
    "### 1.1 Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_label</th>\n",
       "      <th>age_buckets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational_num      marital_status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country  income_label age_buckets  \n",
       "0              40  United-States             0           0  \n",
       "1              50  United-States             0           3  \n",
       "2              40  United-States             1           1  \n",
       "3              40  United-States             1           4  \n",
       "4              30  United-States             0         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following will all happen if you simply run: python prepare_data.py --dataset adult\n",
    "DF_adult = pd.read_csv(DATA_PATH/'adult/adult.csv')\n",
    "DF_adult.columns = [c.replace(\"-\", \"_\") for c in DF_adult.columns]\n",
    "DF_adult['income_label'] = (DF_adult[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "DF_adult.drop(\"income\", axis=1, inplace=True)\n",
    "DF_adult['age_buckets'] = pd.cut(DF_adult.age, bins=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65],\n",
    "    labels=np.arange(9))\n",
    "out_dir = DATA_PATH/'adult/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "wide_cols = ['age_buckets', 'education', 'relationship','workclass','occupation',\n",
    "    'native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "# DEEP DENSE\n",
    "embeddings_cols = [('education',16), ('relationship',16), ('workclass',16),\n",
    "    ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "standardize_cols = continuous_cols\n",
    "\n",
    "#TARGET: logistic\n",
    "target = 'income_label'\n",
    "DF_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide and Deep adult data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_adult = prepare_data_adult(\n",
    "    DF_adult, wide_cols,\n",
    "    crossed_cols,\n",
    "    embeddings_cols,\n",
    "    continuous_cols,\n",
    "    standardize_cols,\n",
    "    target, out_dir,\n",
    "    scale=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test', 'cat_embeddings_input', 'cat_embeddings_encoding_dict', 'continuous_cols', 'deep_column_idx'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset_adult.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wide', 'deep_dense', 'target'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd_dataset_adult['train'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_adult['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_adult['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_adult['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_adult['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_adult['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "model1 = WideDeep(output_dim=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=805, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_education): Embedding(16, 16)\n",
       "    (emb_layer_relationship): Embedding(6, 16)\n",
       "    (emb_layer_workclass): Embedding(9, 16)\n",
       "    (emb_layer_occupation): Embedding(15, 16)\n",
       "    (emb_layer_native_country): Embedding(42, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=82, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compile and Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer={'widedeep': ['Adam', 0.01]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(method='logistic', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 229/229 [00:02<00:00, 114.23it/s, acc=0.83, loss=0.367] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 167.71it/s, acc=0.84, loss=0.356] \n",
      "epoch 2: 100%|██████████| 229/229 [00:01<00:00, 127.67it/s, acc=0.84, loss=0.346] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.71it/s, acc=0.841, loss=0.345]\n",
      "epoch 3: 100%|██████████| 229/229 [00:01<00:00, 128.27it/s, acc=0.842, loss=0.342]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 172.08it/s, acc=0.838, loss=0.348]\n",
      "epoch 4: 100%|██████████| 229/229 [00:01<00:00, 128.06it/s, acc=0.842, loss=0.34] \n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.86it/s, acc=0.84, loss=0.349] \n",
      "epoch 5: 100%|██████████| 229/229 [00:01<00:00, 127.87it/s, acc=0.841, loss=0.339]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.27it/s, acc=0.841, loss=0.351]\n",
      "epoch 6: 100%|██████████| 229/229 [00:01<00:00, 127.71it/s, acc=0.844, loss=0.337]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.53it/s, acc=0.84, loss=0.349] \n",
      "epoch 7: 100%|██████████| 229/229 [00:01<00:00, 128.25it/s, acc=0.843, loss=0.336]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.37it/s, acc=0.841, loss=0.348]\n",
      "epoch 8: 100%|██████████| 229/229 [00:01<00:00, 128.31it/s, acc=0.843, loss=0.336]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 210.34it/s, acc=0.842, loss=0.348]\n",
      "epoch 9: 100%|██████████| 229/229 [00:01<00:00, 127.86it/s, acc=0.844, loss=0.335]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.76it/s, acc=0.84, loss=0.352] \n",
      "epoch 10: 100%|██████████| 229/229 [00:01<00:00, 128.19it/s, acc=0.843, loss=0.335]\n",
      "valid: 100%|██████████| 77/77 [00:00<00:00, 211.48it/s, acc=0.841, loss=0.352]\n"
     ]
    }
   ],
   "source": [
    "train_set = WideDeepLoader(wd_dataset_adult['train'], mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_adult['valid'], mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_adult['test'], mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)\n",
    "model1.fit(n_epochs=10, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression with the airbnb dataset using all: Wide, Deep_Dense, Deep_Text and Deep_Image. Also, multiple optimizers and learning rate schedulers\n",
    "\n",
    "### 2.1. Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assume you have runned airbnb_data_preprocessing.py and the resulting file is at \n",
    "# DATA_PATH/'airbnb/listings_processed.csv'\n",
    "DF_airbnb = pd.read_csv(DATA_PATH/'airbnb/listings_processed.csv')\n",
    "DF_airbnb = DF_airbnb[DF_airbnb.description.apply(lambda x: len(x.split(' '))>=10)]\n",
    "out_dir = DATA_PATH/'airbnb/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "crossed_cols = (['property_type', 'room_type'],)\n",
    "already_dummies = [c for c in DF_airbnb.columns if 'amenity' in c] + ['has_house_rules']\n",
    "wide_cols = ['is_location_exact', 'property_type', 'room_type', 'host_gender'] +\\\n",
    "    already_dummies\n",
    "\n",
    "#DEEP_DENSE\n",
    "embeddings_cols = [(c, 16) for c in DF_airbnb.columns if 'catg' in c] + [('neighbourhood_cleansed', 64)]\n",
    "continuous_cols = ['latitude', 'longitude', 'security_deposit', 'extra_people']\n",
    "standardize_cols = ['security_deposit', 'extra_people']\n",
    "\n",
    "# DEEP_TEXT\n",
    "text_col = 'description'\n",
    "word_vectors_path = 'data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# DEEP_IMAGE\n",
    "img_id = 'id'\n",
    "img_path = DATA_PATH/'airbnb/property_picture'\n",
    "\n",
    "#TARGET\n",
    "target = 'yield'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset, easy with `prepare_data_airbnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images from data/airbnb/property_picture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/5000 [00:00<00:12, 405.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:12<00:00, 384.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 12433 words\n",
      "Indexing word vectors...\n",
      "Loaded 400000 word vectors\n",
      "Preparing embeddings matrix...\n",
      "6776 words in our vocabulary had glove vectors and appear more than the min frequency\n",
      "Wide and Deep airbnb data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_airbnb = prepare_data_airbnb(\n",
    "    # let's use only 5000 observations (not all of them will have images, so we might end with 4900+)\n",
    "    df = DF_airbnb.sample(5000),\n",
    "    img_id = img_id,\n",
    "    img_path = img_path,\n",
    "    text_col = text_col,\n",
    "    max_vocab = 20000,\n",
    "    min_freq = 2,\n",
    "    maxlen = 170,\n",
    "    word_vectors_path = word_vectors_path,\n",
    "    embeddings_cols = embeddings_cols,\n",
    "    continuous_cols = continuous_cols,\n",
    "    standardize_cols = standardize_cols,\n",
    "    target = target,\n",
    "    wide_cols = wide_cols,\n",
    "    crossed_cols = crossed_cols,\n",
    "    already_dummies = already_dummies,\n",
    "    out_dir = out_dir,\n",
    "    scale=True,\n",
    "    seed=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand what all these parameters mean, simple see demo1 and demo2 and the modules in widedeep.models\n",
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_airbnb['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_airbnb['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_airbnb['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_airbnb['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_airbnb['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )\n",
    "params['deep_text'] = dict(\n",
    "    vocab_size = len(wd_dataset_airbnb['vocab'].itos),\n",
    "    embedding_dim = wd_dataset_airbnb['word_embeddings_matrix'].shape[1],\n",
    "    hidden_dim = 64,\n",
    "    n_layers = 2,\n",
    "    rnn_dropout = 0.5,\n",
    "    spatial_dropout = 0.1,\n",
    "    padding_idx = 1,\n",
    "    attention = False,\n",
    "    bidirectional = False,\n",
    "    embedding_matrix = wd_dataset_airbnb['word_embeddings_matrix']\n",
    "    )\n",
    "params['deep_img'] = dict(\n",
    "    pretrained = True,\n",
    "    freeze=6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build the Model\n",
    "\n",
    "The model is built exactly as in the case of the adult dataset before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "model2 = WideDeep(output_dim=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=213, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_neighbourhood_cleansed): Embedding(33, 64)\n",
       "    (emb_layer_bathrooms_catg): Embedding(3, 16)\n",
       "    (emb_layer_host_listings_count_catg): Embedding(4, 16)\n",
       "    (emb_layer_minimum_nights_catg): Embedding(3, 16)\n",
       "    (emb_layer_beds_catg): Embedding(4, 16)\n",
       "    (emb_layer_bedrooms_catg): Embedding(4, 16)\n",
       "    (emb_layer_guests_included_catg): Embedding(3, 16)\n",
       "    (emb_layer_accommodates_catg): Embedding(3, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (deep_text): DeepText(\n",
       "    (embedding_dropout): Dropout2d(p=0.1)\n",
       "    (embedding): Embedding(7093, 300, padding_idx=1)\n",
       "    (rnn): GRU(300, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dtlinear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (deep_img): DeepImage(\n",
       "    (backbone): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (dilinear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compile and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example...\n",
    "optimizer=dict(\n",
    "    wide=['Adam', 0.1],\n",
    "    deep_dense=['Adam', 0.01],\n",
    "    deep_text=['RMSprop', 0.01,0.1],\n",
    "    deep_img= ['Adam', 0.01]\n",
    "    )\n",
    "lr_scheduler=dict(\n",
    "    wide=['StepLR', 3, 0.1],\n",
    "    deep_dense=['StepLR', 3, 0.1],\n",
    "    deep_text=['MultiStepLR', [3,5,7], 0.1],\n",
    "    deep_img=['MultiStepLR', [3,5,7], 0.1]\n",
    "    )\n",
    "# if you want just one optimizer and lr_scheduler call simply\n",
    "# optimizer={'widedeep': ['Adam', 0.01]}\n",
    "# lr_scheduler = {'widedeep': ['StepLR', 3, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(method='regression', optimizer=optimizer, lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2 reads bgr\n",
    "# mean=[0.485, 0.456, 0.406] #RGB\n",
    "# std=[0.229, 0.224, 0.225]  #RGB\n",
    "mean=[0.406, 0.456, 0.485] #RGB\n",
    "std=[0.225, 0.224, 0.229]  #RGB\n",
    "transform  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "train_set = WideDeepLoader(wd_dataset_airbnb['train'], transform, mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_airbnb['valid'], transform, mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_airbnb['test'], transform, mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=64,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=64,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 47/47 [00:22<00:00,  2.05it/s, loss=118]\n",
      "valid: 100%|██████████| 16/16 [00:04<00:00,  4.06it/s, loss=117]\n",
      "epoch 2: 100%|██████████| 47/47 [00:22<00:00,  2.14it/s, loss=105]\n",
      "valid: 100%|██████████| 16/16 [00:04<00:00,  4.20it/s, loss=158]\n",
      "epoch 3: 100%|██████████| 47/47 [00:22<00:00,  2.15it/s, loss=99.2]\n",
      "valid: 100%|██████████| 16/16 [00:03<00:00,  4.27it/s, loss=99]  \n",
      "epoch 4: 100%|██████████| 47/47 [00:22<00:00,  2.13it/s, loss=97]  \n",
      "valid: 100%|██████████| 16/16 [00:04<00:00,  4.06it/s, loss=99.6]\n",
      "epoch 5: 100%|██████████| 47/47 [00:22<00:00,  2.15it/s, loss=94.7]\n",
      "valid: 100%|██████████| 16/16 [00:04<00:00,  4.24it/s, loss=99]  \n"
     ]
    }
   ],
   "source": [
    "model2.fit(n_epochs=5, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 32/32 [00:04<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.88571074249793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(test_loader)\n",
    "y = wd_dataset_airbnb['test']['target']\n",
    "print(np.sqrt(mean_squared_error(y, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Extract the learned embeddings for a given categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood_cleansed_emb = model2.get_embeddings(col_name='neighbourhood_cleansed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hammersmith and Fulham': array([-1.851022,  0.703244,  0.34214 , -1.215795, ..., -0.218133, -0.107084, -0.050649, -1.276854], dtype=float32),\n",
       " 'Barnet': array([-1.739752, -0.935879,  0.335114, -1.109356, ..., -2.491122, -0.632074,  2.492794,  1.231859], dtype=float32),\n",
       " 'Ealing': array([-0.89164 ,  0.005264, -2.254685,  0.572432, ..., -1.358266, -1.876953,  1.084197,  0.825844], dtype=float32),\n",
       " 'Greenwich': array([-0.686692, -0.623536,  1.663162,  1.130035, ..., -0.359084, -0.609575,  0.304735, -1.042224], dtype=float32),\n",
       " 'Lambeth': array([-0.047345, -0.697568,  0.793924, -0.18951 , ...,  0.230893, -0.170741, -0.592736, -0.755723], dtype=float32),\n",
       " 'Lewisham': array([-0.302935,  1.052123,  0.883626,  0.127071, ..., -0.047294, -0.667769,  1.237696,  1.278981], dtype=float32),\n",
       " 'Richmond upon Thames': array([-1.108736,  0.175303, -1.596437, -0.13958 , ...,  0.557685,  0.076416, -0.171436,  1.561785], dtype=float32),\n",
       " 'Wandsworth': array([-0.133121, -1.265229, -0.536881, -0.235154, ...,  0.39987 , -0.759289,  0.188098, -1.317402], dtype=float32),\n",
       " 'Camden': array([ 1.093842, -0.654579,  0.45953 , -1.833444, ...,  0.678493, -0.840447, -0.144676,  0.803519], dtype=float32),\n",
       " 'Southwark': array([ 0.632342, -1.031606, -1.9757  ,  1.434942, ...,  0.477039, -0.716686, -1.574186,  0.361259], dtype=float32),\n",
       " 'Westminster': array([-0.006769,  1.014129,  0.38176 ,  1.087195, ...,  0.619644,  0.145372, -1.583134, -0.53737 ], dtype=float32),\n",
       " 'Newham': array([-0.940997,  0.449212,  0.006719, -0.971067, ..., -1.480452,  1.291778, -2.473881, -0.788751], dtype=float32),\n",
       " 'Tower Hamlets': array([-0.257651, -1.176663, -0.254655,  0.915376, ...,  0.678123, -1.044624,  0.056251, -2.595061], dtype=float32),\n",
       " 'Hackney': array([-0.032634, -1.021449,  0.060701, -0.706772, ...,  1.819459, -0.264873, -0.062177,  0.125787], dtype=float32),\n",
       " 'Merton': array([-0.380734, -0.538981, -0.415401,  1.023716, ...,  1.238271,  1.291368,  0.297376, -0.182454], dtype=float32),\n",
       " 'Haringey': array([-0.39299 , -0.500936, -1.375325,  0.125624, ...,  1.305327,  0.661907,  0.926493, -2.139146], dtype=float32),\n",
       " 'Islington': array([ 0.407774, -0.393032,  0.543974,  0.567474, ..., -0.864082, -0.724516, -0.102573, -1.721182], dtype=float32),\n",
       " 'Havering': array([ 0.012448,  0.703619,  1.464297,  0.258891, ..., -0.021862, -0.373643, -0.002513, -0.207162], dtype=float32),\n",
       " 'Brent': array([-1.178367,  0.598762, -0.947129, -0.834452, ..., -0.257813,  0.121773, -1.224157, -0.314848], dtype=float32),\n",
       " 'Kensington and Chelsea': array([-0.056346, -0.968234,  0.959274, -1.280915, ...,  0.104357,  0.123029,  0.263767, -1.308004], dtype=float32),\n",
       " 'Croydon': array([-0.697593, -0.6681  ,  0.333914,  1.499083, ..., -0.066999, -0.861174,  1.097988,  0.951798], dtype=float32),\n",
       " 'Hounslow': array([-1.200491,  1.092508, -1.106972, -0.012594, ..., -0.200799, -1.20688 , -0.161942,  1.381369], dtype=float32),\n",
       " 'Hillingdon': array([ 0.560095, -1.294285,  1.093274, -0.846406, ..., -1.553291,  0.536398, -0.276216,  1.071183], dtype=float32),\n",
       " 'Enfield': array([ 0.907168, -0.619706,  1.104722,  0.555476, ..., -1.789324, -1.150976, -0.895986,  0.604396], dtype=float32),\n",
       " 'Waltham Forest': array([-1.09497 ,  0.34694 , -2.238035,  0.603189, ..., -0.058768, -1.1416  , -0.313438,  0.146215], dtype=float32),\n",
       " 'Harrow': array([-1.697243, -0.534554, -0.166178,  0.088522, ..., -0.103859, -1.85962 , -0.387001, -0.54297 ], dtype=float32),\n",
       " 'Redbridge': array([-0.439203, -1.083957, -0.588426, -0.761668, ...,  0.055353,  1.498796,  0.979085, -0.972337], dtype=float32),\n",
       " 'Bromley': array([-2.125255, -0.03386 ,  1.223974,  0.211232, ...,  0.094852,  0.507099, -0.205213,  1.35158 ], dtype=float32),\n",
       " 'Sutton': array([ 0.3636  , -0.62571 ,  0.074913, -0.408316, ...,  1.613977,  1.136851, -2.397302, -0.385847], dtype=float32),\n",
       " 'City of London': array([ 1.146366,  0.461671, -1.337096,  0.136036, ...,  0.683757, -0.658017,  0.520523,  0.580623], dtype=float32),\n",
       " 'Barking and Dagenham': array([-2.43159 , -0.656072, -1.209697,  1.669525, ...,  0.124255,  2.291806, -0.741579, -1.083829], dtype=float32),\n",
       " 'Kingston upon Thames': array([-1.791893, -0.873715,  0.819117, -1.091105, ..., -0.759607, -0.174701,  1.495903,  0.518327], dtype=float32),\n",
       " 'Bexley': array([-0.761755, -0.174487, -1.790552, -0.710304, ...,  1.084789,  1.210242,  0.164761,  0.111679], dtype=float32)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbourhood_cleansed_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiclass Classification with the airbnb dataset using Wide, Deep_Dense and Deep_Text with one optimizers and one learning rate scheduler\n",
    "\n",
    "Here we will fake a multiclass classification problem using the target `yield`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set up and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_airbnb['yield_cat'] = pd.cut(DF_airbnb['yield'], bins=[0.2, 65, 163, 600], labels=[0,1,2])\n",
    "DF_airbnb.drop('yield', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is the same\n",
    "out_dir = DATA_PATH/'airbnb/wide_deep_data/'\n",
    "\n",
    "# WIDE\n",
    "crossed_cols = (['property_type', 'room_type'],)\n",
    "already_dummies = [c for c in DF_airbnb.columns if 'amenity' in c] + ['has_house_rules']\n",
    "wide_cols = ['is_location_exact', 'property_type', 'room_type', 'host_gender'] +\\\n",
    "    already_dummies\n",
    "\n",
    "#DEEP_DENSE\n",
    "embeddings_cols = [(c, 16) for c in DF_airbnb.columns if 'catg' in c] + [('neighbourhood_cleansed', 64)]\n",
    "continuous_cols = ['latitude', 'longitude', 'security_deposit', 'extra_people']\n",
    "standardize_cols = ['security_deposit', 'extra_people']\n",
    "\n",
    "# DEEP_TEXT\n",
    "text_col = 'description'\n",
    "word_vectors_path = 'data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "# DEEP_IMAGE\n",
    "img_id = 'id'\n",
    "img_path = DATA_PATH/'airbnb/property_picture'\n",
    "\n",
    "#TARGET\n",
    "target = 'yield_cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images from data/airbnb/property_picture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 41/5000 [00:00<00:12, 402.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:12<00:00, 387.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary contains 12675 words\n",
      "Indexing word vectors...\n",
      "Loaded 400000 word vectors\n",
      "Preparing embeddings matrix...\n",
      "6786 words in our vocabulary had glove vectors and appear more than the min frequency\n",
      "Wide and Deep airbnb data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "wd_dataset_airbnb = prepare_data_airbnb(\n",
    "    # let's use only 5000 observations (not all of them will have images, so we might end with 4900+)\n",
    "    df = DF_airbnb.sample(5000),\n",
    "    img_id = img_id,\n",
    "    img_path = img_path,\n",
    "    text_col = text_col,\n",
    "    max_vocab = 20000,\n",
    "    min_freq = 2,\n",
    "    maxlen = 170,\n",
    "    word_vectors_path = word_vectors_path,\n",
    "    embeddings_cols = embeddings_cols,\n",
    "    continuous_cols = continuous_cols,\n",
    "    standardize_cols = standardize_cols,\n",
    "    target = target,\n",
    "    wide_cols = wide_cols,\n",
    "    crossed_cols = crossed_cols,\n",
    "    already_dummies = already_dummies,\n",
    "    out_dir = out_dir,\n",
    "    scale=True,\n",
    "    seed=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's drop the image dataset, this time \"only\" with Wide, Deep_Dense and Deep_Text\n",
    "del wd_dataset_airbnb['train']['deep_img']\n",
    "del wd_dataset_airbnb['valid']['deep_img']\n",
    "del wd_dataset_airbnb['test']['deep_img']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['wide'] = dict(\n",
    "    wide_dim = wd_dataset_airbnb['train']['wide'].shape[1]\n",
    "    )\n",
    "params['deep_dense'] = dict(\n",
    "    embeddings_input = wd_dataset_airbnb['cat_embeddings_input'],\n",
    "    embeddings_encoding_dict = wd_dataset_airbnb['cat_embeddings_encoding_dict'],\n",
    "    continuous_cols = wd_dataset_airbnb['continuous_cols'],\n",
    "    deep_column_idx = wd_dataset_airbnb['deep_column_idx'],\n",
    "    hidden_layers = [64,32],\n",
    "    dropout = [0.5]\n",
    "    )\n",
    "params['deep_text'] = dict(\n",
    "    vocab_size = len(wd_dataset_airbnb['vocab'].itos),\n",
    "    embedding_dim = wd_dataset_airbnb['word_embeddings_matrix'].shape[1],\n",
    "    hidden_dim = 64,\n",
    "    n_layers = 3,\n",
    "    rnn_dropout = 0.5,\n",
    "    spatial_dropout = 0.1,\n",
    "    padding_idx = 1,\n",
    "    attention = False,\n",
    "    bidirectional = False,\n",
    "    embedding_matrix = wd_dataset_airbnb['word_embeddings_matrix']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widedeep.models.wide_deep import WideDeepLoader, WideDeep\n",
    "# We have 3 classes\n",
    "model3 = WideDeep(output_dim=3, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wlinear): Linear(in_features=213, out_features=3, bias=True)\n",
       "  )\n",
       "  (deep_dense): DeepDense(\n",
       "    (emb_layer_neighbourhood_cleansed): Embedding(33, 64)\n",
       "    (emb_layer_bathrooms_catg): Embedding(3, 16)\n",
       "    (emb_layer_host_listings_count_catg): Embedding(4, 16)\n",
       "    (emb_layer_minimum_nights_catg): Embedding(3, 16)\n",
       "    (emb_layer_beds_catg): Embedding(4, 16)\n",
       "    (emb_layer_bedrooms_catg): Embedding(4, 16)\n",
       "    (emb_layer_guests_included_catg): Embedding(3, 16)\n",
       "    (emb_layer_accommodates_catg): Embedding(3, 16)\n",
       "    (dense): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.0)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01, inplace)\n",
       "        (2): Dropout(p=0.5)\n",
       "      )\n",
       "      (last_linear): Linear(in_features=32, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (deep_text): DeepText(\n",
       "    (embedding_dropout): Dropout2d(p=0.1)\n",
       "    (embedding): Embedding(7097, 300, padding_idx=1)\n",
       "    (rnn): GRU(300, 64, num_layers=3, batch_first=True, dropout=0.5)\n",
       "    (dtlinear): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.4 Compile and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer={'widedeep': ['Adam', 0.01]}\n",
    "lr_scheduler = {'widedeep': ['StepLR', 3, 0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(method='multiclass', optimizer=optimizer, lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WideDeepLoader(wd_dataset_airbnb['train'], mode='train')\n",
    "valid_set = WideDeepLoader(wd_dataset_airbnb['valid'], mode='train')\n",
    "test_set = WideDeepLoader(wd_dataset_airbnb['test'], mode='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set,\n",
    "    batch_size=128,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "    batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wide', 'deep_dense', 'deep_text', 'target']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.input_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 24/24 [00:03<00:00,  7.05it/s, acc=0.547, loss=0.985]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 37.03it/s, acc=0.588, loss=0.949]\n",
      "epoch 2: 100%|██████████| 24/24 [00:03<00:00,  7.96it/s, acc=0.601, loss=0.936]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 41.34it/s, acc=0.58, loss=0.951] \n",
      "epoch 3: 100%|██████████| 24/24 [00:02<00:00,  8.47it/s, acc=0.634, loss=0.904]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 42.72it/s, acc=0.597, loss=0.936]\n",
      "epoch 4: 100%|██████████| 24/24 [00:02<00:00,  8.47it/s, acc=0.663, loss=0.887]\n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 43.66it/s, acc=0.581, loss=0.945]\n",
      "epoch 5: 100%|██████████| 24/24 [00:02<00:00,  8.63it/s, acc=0.67, loss=0.876] \n",
      "valid: 100%|██████████| 8/8 [00:00<00:00, 43.78it/s, acc=0.576, loss=0.943]\n"
     ]
    }
   ],
   "source": [
    "model3.fit(n_epochs=5, train_loader=train_loader, eval_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont pay much attention to the results, this is just an artificial experiments for you to see how one would use it for multiclass classification. \n",
    "\n",
    "And with this, this is it, I guess you now have all the information to run as many experiments as you want combining all sorts of datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jrz)",
   "language": "python",
   "name": "conda_jrz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
