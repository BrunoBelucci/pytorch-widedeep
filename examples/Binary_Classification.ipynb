{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Binary Classification with defaults.\n",
    "\n",
    "In this notebook we will use the Adult Census dataset. Download the data from [here](https://www.kaggle.com/wenruliu/adult-income-dataset/downloads/adult.csv/2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, DeepPreprocessor\n",
    "from pytorch_widedeep.models import Wide, DeepDense, WideDeep\n",
    "from pytorch_widedeep.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult/adult.csv.zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>age_buckets</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational_num      marital_status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital_gain  capital_loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours_per_week native_country age_buckets  income_label  \n",
       "0              40  United-States           0             0  \n",
       "1              50  United-States           3             0  \n",
       "2              40  United-States           1             1  \n",
       "3              40  United-States           4             1  \n",
       "4              30  United-States           0             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, we'll do some mild preprocessing\n",
    "df.columns = [c.replace(\"-\", \"_\") for c in df.columns]\n",
    "df['age_buckets'] = pd.cut(df.age, bins=[16, 25, 30, 35, 40, 45, 50, 55, 60, 91], labels=np.arange(9))\n",
    "df['income_label'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df.drop('income', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preparing the data \n",
    "\n",
    "The so called `Wide` part of the model is simply set of one-hot encoded features connected to the output neuron(s) through a linear layer. \n",
    "\n",
    "The so called `Deep` part, which I will refer here as `DeepDense` (since there will also be `DeepText` and `DeepImage`) is a set of numerical continuous features, concatenated with embedding representations of categorical features, passed through a series of Dense layers. \n",
    "\n",
    "With this in mind, we prepare the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = ['age_buckets', 'education', 'relationship','workclass','occupation',\n",
    "    'native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n",
    "cat_embed_cols = [('education',16), ('relationship',8), ('workclass',16),\n",
    "    ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target_col = 'income_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET\n",
    "target = df[target_col].values\n",
    "\n",
    "# WIDE\n",
    "preprocess_wide = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = preprocess_wide.fit_transform(df)\n",
    "\n",
    "# DEEP\n",
    "preprocess_deep = DeepPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_deep = preprocess_deep.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(48842, 805)\n"
     ]
    }
   ],
   "source": [
    "print(X_wide)\n",
    "print(X_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.         -0.99512893\n",
      "  -0.03408696]\n",
      " [ 1.          1.          0.         ...  0.         -0.04694151\n",
      "   0.77292975]\n",
      " [ 2.          1.          1.         ...  0.         -0.77631645\n",
      "  -0.03408696]\n",
      " ...\n",
      " [ 1.          3.          0.         ...  0.          1.41180837\n",
      "  -0.03408696]\n",
      " [ 1.          0.          0.         ...  0.         -1.21394141\n",
      "  -1.64812038]\n",
      " [ 1.          4.          6.         ...  0.          0.97418341\n",
      "  -0.03408696]]\n",
      "(48842, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_deep)\n",
    "print(X_deep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=X_wide.shape[1], output_dim=1)\n",
    "deepdense = DeepDense(hidden_layers=[64,32], \n",
    "                      deep_column_idx=preprocess_deep.deep_column_idx,\n",
    "                      embed_input=preprocess_deep.embeddings_input,\n",
    "                      continuous_cols=continuous_cols)\n",
    "model = WideDeep(wide=wide, deepdense=deepdense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wide_linear): Linear(in_features=805, out_features=1, bias=True)\n",
       "  )\n",
       "  (deepdense): Sequential(\n",
       "    (0): DeepDense(\n",
       "      (embed_layers): ModuleDict(\n",
       "        (emb_layer_education): Embedding(16, 16)\n",
       "        (emb_layer_native_country): Embedding(42, 16)\n",
       "        (emb_layer_occupation): Embedding(15, 16)\n",
       "        (emb_layer_relationship): Embedding(6, 8)\n",
       "        (emb_layer_workclass): Embedding(9, 16)\n",
       "      )\n",
       "      (dense): Sequential(\n",
       "        (dense_layer_0): Sequential(\n",
       "          (0): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (dense_layer_1): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model is not particularly complex. In mathematical terms (Eq 3 in the [original paper](https://arxiv.org/pdf/1606.07792.pdf)): \n",
    "\n",
    "$$\n",
    "pred = \\sigma(W^{T}_{wide}[x, \\phi(x)] + W^{T}_{deep}a_{deep}^{(l_f)} +  b) \n",
    "$$ \n",
    "\n",
    "\n",
    "The architecture above will output the 1st and the second term in the parenthesis. `WideDeep` will then add them and apply and activation function (`sigmoid` in this case). For more details, please refer to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compiling and Running/Fitting\n",
    "\n",
    "Once the model is built, we just need to compile it and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(method='binary', metrics=[BinaryAccuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:01<00:00, 88.02it/s, loss=0.404, metrics={'acc': 0.812}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 140.39it/s, loss=0.356, metrics={'acc': 0.8165}]\n",
      "epoch 2: 100%|██████████| 153/153 [00:01<00:00, 91.37it/s, loss=0.348, metrics={'acc': 0.8357}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 122.80it/s, loss=0.35, metrics={'acc': 0.8361}]\n",
      "epoch 3: 100%|██████████| 153/153 [00:01<00:00, 89.79it/s, loss=0.343, metrics={'acc': 0.8382}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 97.31it/s, loss=0.348, metrics={'acc': 0.8382}]\n",
      "epoch 4: 100%|██████████| 153/153 [00:01<00:00, 96.99it/s, loss=0.34, metrics={'acc': 0.8398}]  \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 137.56it/s, loss=0.348, metrics={'acc': 0.8395}]\n",
      "epoch 5: 100%|██████████| 153/153 [00:01<00:00, 97.95it/s, loss=0.338, metrics={'acc': 0.8411}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 133.39it/s, loss=0.347, metrics={'acc': 0.8406}]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_wide=X_wide, X_deep=X_deep, target=target, n_epochs=5, batch_size=256, val_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you can run a wide and deep model in slightly over a dozen lines of code:\n",
    "\n",
    "```\n",
    "wide_cols = ['age_buckets', 'education', 'relationship','workclass','occupation', 'native_country','gender']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]\n",
    "cat_embed_cols = [('education',16), ('relationship',8), ('workclass',16),\n",
    "    ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target_col = 'income_label'\n",
    "target = df[target_col].values\n",
    "preprocess_wide = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = preprocess_wide.fit_transform(df)\n",
    "preprocess_deep = DeepPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_deep = preprocess_deep.fit_transform(df)\n",
    "wide = Wide(wide_dim=X_wide.shape[1], output_dim=1)\n",
    "deepdense = DeepDense(hidden_layers=[64,32], \n",
    "                      deep_column_idx=preprocess_deep.deep_column_idx,\n",
    "                      embed_input=preprocess_deep.embeddings_input,\n",
    "                      continuous_cols=continuous_cols)\n",
    "model = WideDeep(wide=wide, deepdense=deepdense)\n",
    "model.compile(method='binary', metrics=[BinaryAccuracy])\n",
    "model.fit(X_wide=X_wide, X_deep=X_deep, target=target, n_epochs=5, batch_size=256, val_split=0.2)\n",
    "```\n",
    "\n",
    "Let's now see how to use WideDeep with varying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification with varying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(wide_dim=X_wide.shape[1], output_dim=1)\n",
    "# We can add dropout and batchnorm to the dense layers\n",
    "deepdense = DeepDense(hidden_layers=[64,32], dropout=[0.5, 0.5], batchnorm=True,\n",
    "                      deep_column_idx=preprocess_deep.deep_column_idx,\n",
    "                      embed_input=preprocess_deep.embeddings_input,\n",
    "                      continuous_cols=continuous_cols)\n",
    "model = WideDeep(wide=wide, deepdense=deepdense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wide_linear): Linear(in_features=805, out_features=1, bias=True)\n",
       "  )\n",
       "  (deepdense): Sequential(\n",
       "    (0): DeepDense(\n",
       "      (embed_layers): ModuleDict(\n",
       "        (emb_layer_education): Embedding(16, 16)\n",
       "        (emb_layer_native_country): Embedding(42, 16)\n",
       "        (emb_layer_occupation): Embedding(15, 16)\n",
       "        (emb_layer_relationship): Embedding(6, 8)\n",
       "        (emb_layer_workclass): Embedding(9, 16)\n",
       "      )\n",
       "      (dense): Sequential(\n",
       "        (dense_layer_0): Sequential(\n",
       "          (0): Linear(in_features=74, out_features=64, bias=True)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (dense_layer_1): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different initializers, optimizers and learning rate schedulers for each `branch` of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep.initializers import KaimingNormal, XavierNormal\n",
    "from pytorch_widedeep.callbacks import ModelCheckpoint, LRHistory, EarlyStopping\n",
    "from pytorch_widedeep.optim import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "wide_opt = torch.optim.Adam(model.wide.parameters())\n",
    "deep_opt = RAdam(model.deepdense.parameters())\n",
    "# LR Schedulers\n",
    "wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=3)\n",
    "deep_sch = torch.optim.lr_scheduler.StepLR(deep_opt, step_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the components that are model dependent must be passed as dictionaries, while general components are simply lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {'wide': wide_opt, 'deepdense':deep_opt}\n",
    "schedulers = {'wide': wide_sch, 'deepdense':deep_sch}\n",
    "initializers = {'wide': KaimingNormal, 'deepdense':XavierNormal}\n",
    "callbacks = [LRHistory(n_epochs=10), EarlyStopping, ModelCheckpoint(filepath='model_weights/wd_out')]\n",
    "metrics = [BinaryAccuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(method='binary', optimizers=optimizers, lr_schedulers=schedulers, \n",
    "              initializers=initializers,\n",
    "              callbacks=callbacks,\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 153/153 [00:01<00:00, 77.95it/s, loss=0.618, metrics={'acc': 0.6819}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 141.69it/s, loss=0.417, metrics={'acc': 0.7088}]\n",
      "epoch 2: 100%|██████████| 153/153 [00:02<00:00, 73.45it/s, loss=0.453, metrics={'acc': 0.7875}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 141.03it/s, loss=0.375, metrics={'acc': 0.7954}]\n",
      "epoch 3: 100%|██████████| 153/153 [00:02<00:00, 73.42it/s, loss=0.407, metrics={'acc': 0.8123}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 140.30it/s, loss=0.362, metrics={'acc': 0.8159}]\n",
      "epoch 4: 100%|██████████| 153/153 [00:02<00:00, 76.28it/s, loss=0.385, metrics={'acc': 0.8197}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 124.08it/s, loss=0.357, metrics={'acc': 0.8224}]\n",
      "epoch 5: 100%|██████████| 153/153 [00:02<00:00, 76.28it/s, loss=0.374, metrics={'acc': 0.8247}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 136.63it/s, loss=0.355, metrics={'acc': 0.8261}]\n",
      "epoch 6: 100%|██████████| 153/153 [00:02<00:00, 71.99it/s, loss=0.37, metrics={'acc': 0.8273}] \n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 131.62it/s, loss=0.355, metrics={'acc': 0.8284}]\n",
      "epoch 7: 100%|██████████| 153/153 [00:02<00:00, 72.68it/s, loss=0.368, metrics={'acc': 0.8292}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 136.75it/s, loss=0.354, metrics={'acc': 0.8301}]\n",
      "epoch 8: 100%|██████████| 153/153 [00:02<00:00, 73.58it/s, loss=0.368, metrics={'acc': 0.8272}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 133.58it/s, loss=0.354, metrics={'acc': 0.8284}]\n",
      "epoch 9: 100%|██████████| 153/153 [00:02<00:00, 72.75it/s, loss=0.367, metrics={'acc': 0.8289}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 134.23it/s, loss=0.354, metrics={'acc': 0.8297}]\n",
      "epoch 10: 100%|██████████| 153/153 [00:02<00:00, 70.52it/s, loss=0.366, metrics={'acc': 0.8269}]\n",
      "valid: 100%|██████████| 39/39 [00:00<00:00, 122.14it/s, loss=0.353, metrics={'acc': 0.8283}]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_wide=X_wide, X_deep=X_deep, target=target, n_epochs=10, batch_size=256, val_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activation_fn',\n",
       " '_apply',\n",
       " '_backend',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_construct',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_loss_fn',\n",
       " '_lr_scheduler_step',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_predict',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tracing_name',\n",
       " '_train_val_split',\n",
       " '_training_step',\n",
       " '_validation_step',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'batch_size',\n",
       " 'buffers',\n",
       " 'callback_container',\n",
       " 'callbacks',\n",
       " 'children',\n",
       " 'class_weight',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'cyclic',\n",
       " 'deepdense',\n",
       " 'deephead',\n",
       " 'deepimage',\n",
       " 'deeptext',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'early_stop',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fit',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_embeddings',\n",
       " 'half',\n",
       " 'history',\n",
       " 'initializer',\n",
       " 'load_state_dict',\n",
       " 'lr_history',\n",
       " 'lr_scheduler',\n",
       " 'method',\n",
       " 'metric',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'optimizer',\n",
       " 'parameters',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train_running_loss',\n",
       " 'training',\n",
       " 'transforms',\n",
       " 'type',\n",
       " 'valid_running_loss',\n",
       " 'verbose',\n",
       " 'wide',\n",
       " 'with_focal_loss',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that, among many methods and attributes we have the `history` and `lr_history` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6175944029895308,\n",
       "  0.45259543707947325,\n",
       "  0.4069893552976496,\n",
       "  0.38474080356117946,\n",
       "  0.3735188660668392,\n",
       "  0.37029798708710016,\n",
       "  0.36808953779974796,\n",
       "  0.3682427739395815,\n",
       "  0.36711323962492104,\n",
       "  0.36631106590133866],\n",
       " 'train_acc': [0.6819,\n",
       "  0.7875,\n",
       "  0.8123,\n",
       "  0.8197,\n",
       "  0.8247,\n",
       "  0.8273,\n",
       "  0.8292,\n",
       "  0.8272,\n",
       "  0.8289,\n",
       "  0.8269],\n",
       " 'val_loss': [0.4171598324408898,\n",
       "  0.3752642174561818,\n",
       "  0.36203143841181046,\n",
       "  0.35711457561223936,\n",
       "  0.3552806025896317,\n",
       "  0.35450722697453624,\n",
       "  0.3542046585144141,\n",
       "  0.3536104666881072,\n",
       "  0.3536183413786766,\n",
       "  0.35328011482189864],\n",
       " 'val_acc': [0.7088,\n",
       "  0.7954,\n",
       "  0.8159,\n",
       "  0.8224,\n",
       "  0.8261,\n",
       "  0.8284,\n",
       "  0.8301,\n",
       "  0.8284,\n",
       "  0.8297,\n",
       "  0.8283]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr_wide_0': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  1.0000000000000003e-05,\n",
       "  1.0000000000000003e-05,\n",
       "  1.0000000000000003e-05,\n",
       "  1.0000000000000002e-06],\n",
       " 'lr_deepdense_0': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lr_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learning rate effectively decreases by a factor of 0.1 (the default) after the corresponding `step_size`. Note that the keys of the dictionary have a suffix `_0`. This is because if you pass different parameter groups to the torch optimizers, these will also be recorded. We'll see this in the `Regression` notebook. \n",
    "\n",
    "And I guess one has a good idea of how to use the package. Before we leave this notebook just mentioning that the `WideDeep` class comes with a useful method to \"rescue\" the learned embeddings. For example, let's say I want to use the embeddings learned for the different levels of the categorical feature `education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11th': array([-0.08673421, -0.20611182,  0.59797794,  0.20942385, -0.1655451 ,\n",
       "        -0.35815904, -0.18617755, -0.03302404,  0.35232136,  0.04572977,\n",
       "         0.3746049 , -0.17023513, -0.28413588, -0.0887626 , -0.131666  ,\n",
       "        -0.00224428], dtype=float32),\n",
       " 'HS-grad': array([ 0.52553725,  0.02418377,  0.01348715,  0.08806376,  0.15482062,\n",
       "         0.3125567 , -0.04339381,  0.20687783, -0.13482526,  0.24505831,\n",
       "        -0.01041402, -0.01626713, -0.25524345,  0.0543058 , -0.00122186,\n",
       "         0.17666139], dtype=float32),\n",
       " 'Assoc-acdm': array([ 0.203534  ,  0.04936524,  0.03697095,  0.08620811, -0.22591479,\n",
       "         0.03520108,  0.05318382, -0.15239732, -0.27075678, -0.12126133,\n",
       "         0.2704458 , -0.15518834,  0.24027239,  0.05669733, -0.3112209 ,\n",
       "         0.03612295], dtype=float32),\n",
       " 'Some-college': array([-0.08446952,  0.36122507, -0.33226198,  0.35516104, -0.09688068,\n",
       "         0.03067617, -0.24075967,  0.06579152, -0.27671552,  0.57062864,\n",
       "         0.27405295,  0.04844938, -0.00269691,  0.18688592,  0.11911902,\n",
       "         0.09208092], dtype=float32),\n",
       " '10th': array([ 0.06601457,  0.2555915 ,  0.00195543, -0.03135568,  0.58352757,\n",
       "        -0.16589479, -0.14586753, -0.277025  , -0.03504325,  0.0576626 ,\n",
       "        -0.43172732,  0.15287586, -0.02912221, -0.12770993, -0.05982654,\n",
       "        -0.26651886], dtype=float32),\n",
       " 'Prof-school': array([-0.19953112,  0.08882449,  0.13965775, -0.2410786 ,  0.32581887,\n",
       "         0.3123011 ,  0.25541356, -0.2714716 , -0.04670933,  0.01927072,\n",
       "        -0.13252194,  0.30891562,  0.16324162,  0.4219687 , -0.03921702,\n",
       "        -0.25145957], dtype=float32),\n",
       " '7th-8th': array([ 0.46913517, -0.33059672,  0.10896073,  0.12798366, -0.27347258,\n",
       "        -0.04681386,  0.35597602,  0.38485387,  0.19687116,  0.3415925 ,\n",
       "         0.22646986, -0.13330284, -0.10571367, -0.46806738, -0.09176444,\n",
       "         0.36226785], dtype=float32),\n",
       " 'Bachelors': array([-0.22269225,  0.00756512, -0.137159  , -0.1103561 , -0.30639094,\n",
       "        -0.13993822, -0.01795617, -0.0628717 ,  0.3550157 , -0.26072058,\n",
       "         0.28025642, -0.3055849 ,  0.32886648, -0.21652085,  0.15644898,\n",
       "         0.41821623], dtype=float32),\n",
       " 'Masters': array([ 0.19439612,  0.1160659 ,  0.3101758 , -0.13043888,  0.09493288,\n",
       "         0.16234386, -0.24590236, -0.08965702,  0.2605668 , -0.22599591,\n",
       "         0.29144278,  0.06911745,  0.10599293,  0.24508122, -0.19107586,\n",
       "         0.13113818], dtype=float32),\n",
       " 'Doctorate': array([-0.2663387 ,  0.18198575, -0.00404136, -0.25603142, -0.3210647 ,\n",
       "        -0.29730645,  0.01126224, -0.07884179,  0.05026541, -0.2140362 ,\n",
       "         0.28634158, -0.13386114,  0.1555189 , -0.18269666, -0.10004734,\n",
       "        -0.08088516], dtype=float32),\n",
       " '5th-6th': array([ 0.04861642, -0.1642403 , -0.04409156, -0.01563015,  0.04884195,\n",
       "         0.29488212, -0.10464378, -0.143431  , -0.09019954,  0.36320725,\n",
       "        -0.03127728,  0.17418706, -0.09051888, -0.28603816, -0.04363273,\n",
       "         0.475826  ], dtype=float32),\n",
       " 'Assoc-voc': array([-0.20915316, -0.5593859 ,  0.29052234,  0.0107802 ,  0.42248   ,\n",
       "        -0.15418035,  0.09603997, -0.0274756 ,  0.28711867,  0.02019949,\n",
       "        -0.15214585, -0.11886901,  0.23806143,  0.12080869,  0.08064302,\n",
       "        -0.31949103], dtype=float32),\n",
       " '9th': array([ 0.22722214, -0.37304404,  0.2267464 , -0.17933506,  0.85978544,\n",
       "         0.01301033, -0.04842237,  0.32544634, -0.0987929 ,  0.11031279,\n",
       "        -0.02386631, -0.26282632,  0.36186254, -0.0458326 , -0.03443535,\n",
       "         0.20161769], dtype=float32),\n",
       " '12th': array([ 0.00583783,  0.02599204,  0.35536927,  0.13067342, -0.03334075,\n",
       "         0.2702815 , -0.08792042,  0.21205093,  0.11631412,  0.03543843,\n",
       "         0.06883772, -0.09146094, -0.500762  , -0.13009857,  0.03568175,\n",
       "        -0.26005262], dtype=float32),\n",
       " '1st-4th': array([ 0.02577134,  0.11590238,  0.51769054,  0.0592326 ,  0.00938126,\n",
       "        -0.30967444, -0.24826968, -0.3486966 , -0.24891503,  0.64768374,\n",
       "        -0.2103618 ,  0.04259953,  0.28559977,  0.04214123, -0.2803711 ,\n",
       "         0.28223094], dtype=float32),\n",
       " 'Preschool': array([-0.00465018, -0.00788885, -0.60834825,  0.01634232,  0.00194997,\n",
       "        -0.2870657 ,  0.35325876,  0.3008532 , -0.07418624,  0.04521364,\n",
       "        -0.08075953, -0.14145018,  0.15848197, -0.47788692,  0.44512022,\n",
       "         0.5275078 ], dtype=float32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_embeddings(col_name='education', cat_encoding_dict=preprocess_deep.encoding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
