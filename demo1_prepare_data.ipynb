{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE THE DATA\n",
    "\n",
    "These are the steps required to prepare the data before is passed to the *\"Wide and Deep\"* model at `wide_deep/torch_model.py`\n",
    "\n",
    "Let's first load the data and create a target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DF = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# Let's create a feature that will be our target for logistic regression\n",
    "DF['income_label'] = (DF[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Set the experiment\n",
    "\n",
    "We need to define the columns in the dataset that will be passed to the *\"wide-\"* and the *\"deep-side\"* of the model. For more details of what I mean by \"wide\" and \"deep\" I recommend either to read [this tutorial](https://www.tensorflow.org/tutorials/wide_and_deep), the [original paper](https://arxiv.org/pdf/1606.07792.pdf) or the demo2 in this repo. \n",
    "\n",
    "In the example below, the wide and crossed column will be passed to the wide side of the model while the embedding columns and continuous columns will go through the deep side. \n",
    "\n",
    "We also need to state our target and the method that will be used to fit/predict that target (regression, logistic or multiclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_cols = ['age','hours_per_week','education', 'relationship','workclass',\n",
    "             'occupation','native_country','gender']\n",
    "crossed_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "embeddings_cols = [('education',10), ('relationship',8), ('workclass',10),\n",
    "                    ('occupation',10),('native_country',12)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n",
    "target = 'income_label'\n",
    "method = 'logistic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that `embeddings_cols` is a list of tuples with two elements. These are the column name and the \"dimension of the corresponding embeddings\" (i.e. the number of embeddings per feature), so that when passed through the Deep-side education will be represented by 10 embeddings, relatioship by 8, etc.\n",
    "\n",
    "If you want to use the same number of embeddings for *all* the embedding columns you can simply include the column names and define the number of embeddings when calling to the `prepare_data` function I mention before. This function has a parameter called `def_dim` (default dimension) that will be applied to all embedding columns if no embedding dimension. The first few lines on `prepare_data` look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If embeddings_cols does not include the embeddings dimensions it will be set as\n",
    "# def_dim\n",
    "if type(embeddings_cols[0]) is tuple:\n",
    "    emb_dim = dict(embeddings_cols)\n",
    "    embeddings_cols = [emb[0] for emb in embeddings_cols]\n",
    "else:\n",
    "    emb_dim = {e:def_dim for e in embeddings_cols}\n",
    "deep_cols = embeddings_cols+continuous_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Cross-product for binary features\n",
    "\n",
    "At explained in the original paper: *\"For binary features, a cross-product transformation (e.g.,\n",
    "`AND(gender=female, language=en))` is 1 if and only if the constituent features (`gender=female and language=en`)\n",
    "are all 1, and 0 otherwise\"*. Here, this is implemented by combining the features into a new feature and one-hot encoded it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(DF[target])\n",
    "# We copy the original dataset so we do not mutate it\n",
    "df_tmp = DF.copy()[list(set(wide_cols + deep_cols))]\n",
    "\n",
    "# Build the crossed columns\n",
    "crossed_columns = []\n",
    "for cols in crossed_cols:\n",
    "    colname = '_'.join(cols)\n",
    "    df_tmp[colname] = df_tmp[cols].apply(lambda x: '-'.join(x), axis=1)\n",
    "    crossed_columns.append(colname)\n",
    "\n",
    "# Extract the categorical column names that can be one hot encoded later\n",
    "categorical_columns = list(df_tmp.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to one of the \"crossed features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['education_occupation'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we one-hot encode this feature later, it will be only 1 *if and only* if the two constituent features are 1. In other words, the level `Bachelors-Adm-clerical` of the `education_occupation` feature will be 1 *if and only if* for that particular observation `education=Bachelors` AND `occupation=Adm-clerical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Label-encoding and splitting the dataframe into wide and deep.\n",
    "\n",
    "We first encode the dataframe and keep a dictionary of the encodings for those columns that will be represented as embeddings (for the remaining ones is unneccesary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df, cols=None):\n",
    "    \"\"\"\n",
    "    Helper function to label-encode some features of a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    df  (pd.Dataframe)\n",
    "    cols (list): optional - columns to be label-encoded\n",
    "\n",
    "    Returns:\n",
    "    ________\n",
    "    val_to_idx (dict) : Dictionary of dictionaries with useful information about\n",
    "    the encoding mapping\n",
    "    df (pd.Dataframe): mutated df with Label-encoded features.\n",
    "    \"\"\"\n",
    "\n",
    "    if cols == None:\n",
    "        cols = list(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    return val_to_idx, df\n",
    "\n",
    "# Encode the dataframe and get the encoding Dictionary only for the\n",
    "# deep_cols (for the wide_cols is uneccessary)\n",
    "encoding_dict,df_tmp = label_encode(df_tmp)\n",
    "encoding_dict = {k:encoding_dict[k] for k in encoding_dict if k in deep_cols}\n",
    "embeddings_input = []\n",
    "for k,v in encoding_dict.items():\n",
    "    embeddings_input.append((k, len(v), emb_dim[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data frame into the wide and deep data frames and keep the index of the deep column. This information will be used later since we will slice the tensors based on index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the deep_cols and get the column index that will be use later\n",
    "# to slice the tensors\n",
    "df_deep = df_tmp[deep_cols].copy()\n",
    "deep_column_idx = {k:v for v,k in enumerate(df_deep.columns)}\n",
    "\n",
    "# The continous columns will be concatenated with the embeddings, so you\n",
    "# might want to normalize them first\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for cc in continuous_cols:\n",
    "    df_deep[cc] = scaler.fit_transform(df_deep[cc].values.reshape(-1,1).astype(float))\n",
    "df_wide = df_tmp[wide_cols+crossed_columns]#.copy()\n",
    "del(df_tmp)\n",
    "dummy_cols = [c for c in wide_cols+crossed_columns if c in categorical_columns]\n",
    "df_wide = pd.get_dummies(df_wide, columns=dummy_cols)#.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Train/Test split and build the output dictionary\n",
    "\n",
    "I think the code here is self explanatory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import namedtuple\n",
    "\n",
    "seed = 1981\n",
    "X_train_deep, X_test_deep = train_test_split(df_deep.values, test_size=0.3, random_state=seed)\n",
    "X_train_wide, X_test_wide = train_test_split(df_wide.values, test_size=0.3, random_state=seed)\n",
    "y_train, y_test = train_test_split(Y, test_size=0.3, random_state=1981)\n",
    "\n",
    "# Building the output dictionary\n",
    "wd_dataset = dict()\n",
    "train_dataset = namedtuple('train_dataset', 'wide, deep, labels')\n",
    "test_dataset  = namedtuple('test_dataset' , 'wide, deep, labels')\n",
    "wd_dataset['train_dataset'] = train_dataset(X_train_wide, X_train_deep, y_train)\n",
    "wd_dataset['test_dataset']  = test_dataset(X_test_wide, X_test_deep, y_test)\n",
    "wd_dataset['embeddings_input']  = embeddings_input\n",
    "wd_dataset['deep_column_idx'] = deep_column_idx\n",
    "wd_dataset['encoding_dict'] = encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wd_dataset['train_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wd_dataset['embeddings_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wd_dataset['deep_column_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_dataset['encoding_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emphasize again that all this is wrapped-up in a function saved in the module `wide_deep.data_utils`. Therefore, as long as your data is in a state similar to the original `DF` at the beginning of this notebook, you will be able to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wide_deep.data_utils import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simply call the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
